\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx,tabularx}
\usepackage[a4paper]{geometry}
\usepackage{url}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{verbatim}

\setlength{\parindent}{0pt}
\setlength{\parskip}{\baselineskip}

\title{A music player user interface based on head-gestures and 3D audio feedback}
\author{Jonas Hinge}
\date{June 2014}

\begin{document}

\maketitle

\section{Background}
Music applications on smartphones makes listening to your favourite music very accessible and mobile. Although the possibility of listening to music at any time and place immediately seems like a positive development this could introduce other challenges. E.g. biking and controlling a music application will conflict in the sense that biking demands hands on the handlebars and eyes on the road, and a smartphone application demands hands (or at least one hand) and eyes for navigating resulting in an increase of the users cognitive load.

At the same time emerging accessories with built in sensor hardware e.g. Google Glass or Intelligent Headset (http://intelligentheadset.com/developer/) offer alternate ways of using gestures in form of GPS location, rotation, acceleration, speech etc.

Encouraged by the biking scenario challenge and todays emerging mobile technology - alternative ways of controlling a music application should be explored. In this project an alternative way of navigating using head gestures and audio feedback will be explored.

\begin{comment}
PROJECT FORMULATION FROM COURSE BASE

Smartphone interfaces often require use of the hands and eyes in form of gesture navigating through menus and application user interfaces. In some everyday scenarios however it is challenging to navigate the smartphone in this traditional way e.g. when biking as this requires steering and eyes on the road. Emerging accessories with built in sensor hardware e.g. Google Glass or Intelligent Headset (http://intelligentheadset.com/developer/) offer alternate ways of using gestures and getting feedback when interacting with the music player. In this project it will be investigated how and to which degree the use of head gestures in combination with spatial audio feedback could be used to navigate and control through a music library. 

More specifically the following questions should be answered: 
- Can a user interface based on head gestures and 3d audio compete with existing user interfaces for music players (e.g. touch and vision-based) with respect to for instance a) navigation and control efficiency b) learnability, c) general usability (cognitive/perceptive load), c) suitability to real-world hands-occupied situations. 

- With the chosen combination of input and output modalities, there is a high risk for the system to misinterpret normal everyday actions performed by the user as commands for controlling the system ("behavioural cluttering" (Janlert et al., in press)). How can features in the user interface prevent unwanted manipulation of the system?
\end{comment}

\clearpage

\tableofcontents

\clearpage

\section{Introduction}
Motivation : Use references to claim that using eyes and hands as interaction could be a potential problem in some scenarios or that it could be preferred to use eyes- and handsfree interaction model instead...

Pascoe et al. investigated HCI issues when people are on the move and trials showed that a vital factor was to minimize the amount of distraction for interaction modes \cite{pascoe_using_2000}.

Visual displays can be obtrusive and hard to use in bright daylight, plus they occupy the users’ visual attention \cite{geelhoed_safety_2000}

Short description of what and in which order this project will be executed. What is in the report?


\section{Related work}
Venn diagram
- Music listening: mobile, cognitive load, audio feedback, interaction
- Mobile HCI: cognitive load
- People biking: cognitive load, on the move, hands busy, eyes busy
- Audio feedback
- Gesture based interaction
- Cognitive load
- Multimodal interaction

- Multimodal interaction: head gestures, audio feedback, hands-free, eyes-free, cognitive load effect
- UI learnability

What to compare:
normal interaction vs alternative (head gesture, audio feedback)
Suggestion:
- Normal interaction: Well-known HCI, mobile applications, cognitive load
- Hands- and eyesfree interaction: Learning curve, hands-free, eyes-free, mobile applications, cognitive load
- On-the-move (e.g. biking): distraction, eyes-free, hands-free, cognitive load

Kajastila and Lokki has done a user study comparing auditory and visual menus controlled by the same free-hand gestures where the majority of the participants felt that an auditory circular menu was faster than a visual based menu \cite{kajastila_interaction_2013}.

Brewster et al. showed that novel interaction techniques based on sound and gesture can significantly improve the usability of a wearable device in particular under "eyes-free" mobile conditions and that head gestures was a successful interaction technique with egocentric sounds the most effective \cite{brewster_multimodaleyes-freeinteraction_2003}.

William W. Gaver, a pioneer in audio interfaces, has explored several aspects of using sound in interfaces including the intuitiveness of presenting complex information to users in the form of audio \cite{gaver_sonicfinder:_1989}. Similarly Graham explores the advantages in reaction time when using ”auditory icons” \cite{graham_use_1999}. In \cite{gaver_auditory_1986} Gaver presents the use of spatial sound icons. In doing so, he draws forward the unutilized potential of creating natural interaction through spatial audio.

TODO: Table summing up references that handles the different topics


\section{Interaction design}

\subsection{Auditory menu}
Several studies show that circular auditory menus are the way to go because of horizontally positioned sounds 

\subsection{Multimodal interaction}
Research area in HCI (Human Computer Interaction)

\subsection{3D audio feedback}
HRTF, pilot example from pervasive project

\subsection{Music player interaction design}
Idea: Nod/shake -> yes/no reference (ref from Diako paper)


\section{Implementation}

\subsection{NOTES}

Milestones - POC:
\begin{enumerate}
\item Create and setup xcode project including lib references for Intelligent Headset and Spotify
\item Play a users Spotify songs and be able to use panning and volume
\item Place multiple songs in the horizontal range 0-180 degrees of the user using IHS rotation
\item Implement nodding and shaking feedback
\end{enumerate}

Milestones - further improvements (dependent on user interaction feedback / research):
\begin{enumerate}
\item Exploring the Spotify lib/playlists
\item Activation of head gestures
\item Fine tuning
\end{enumerate}

Learnings:

Streaming from Spotify to OpenAL could be difficult. It requires decompressing of the stream; 1) maybe not possible with libspotify, 2) If so very heavy - could affect user experience, 3) OpenAL restrictions

Idea (avoiding OpenAL's limited audio input properties):

As we only need horizontal 180 degrees we could use panning. Place x songs from 0-180 degrees. As the user rotates head a song pans in from the direction with increasing volume.



\subsection{Application design}
SDK's, APIs, Processing sensor data

\section{Evaluation}

\subsection{NOTES}

Iterations, measurable comparison between new system and traditional?

2 evaluations - closed lab (1 day) and open (real life, week(s))

Idea for closed lab exercise - Multiple lists of songs. A user shouls navigate and play the different songs with head gestures and normal navigation. Compare these in relation to time taken, cognitive load (eyes and at least one hand occupied), user feel of frustration (cognitive load) when navigating

Final evaluation:

Idea: Time to find a song, level of frustration (cognitive load) for finding song

NB: For final evaluation - device with 3G+ connection and added to apple developer team, should be executed latest mid of April so finished end of April (1/2 weeks trial), 2 testpersons -> 1 experienced tech person and 1 non-tech/average user

\section{Discussion}
Other scenarios e.g. visual impaired people, car driving


\section{Conclusion}


%\begin{figure}[H]
%\begin{center}
%\includegraphics[scale=0.45]{graph_4clique.png}
%\end{center}
%caption{\small {\it {4 clique example}}} 
%\label{fig: 4clique example}
%\end{figure}

\clearpage

\bibliography{references}
\bibliographystyle{plain}


\end{document}




