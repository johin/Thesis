\lhead{\emph{Related work}}
\chapter{Related work}
\label{sec:relatedwork}
This chapter will give an overview of the key concepts and systems related to this thesis focus. As mentioned in \ref{sec:goal} the goal of this thesis is to design a music player interface based on head gestures and 3D audio that is suitable for for an interaction while in motion scenario like biking. Two research areas covers a great part of this kind of mobile interaction and they are presented in this chapter; Mobile HCI and Multimodal Interaction. Finally specific mobile systems including mobile music players, that uses related modalities like gestures and audio, are presented in the last section. A graphical overview is shown in fig. \ref{fig:venn}.

\begin{figure}[t]
	\centering
		\includegraphics[width=0.5\textwidth,height=\textheight,keepaspectratio]{./Figures/venn.png}
		\rule{35em}{0.5pt}
	\caption[Venn diagram]{Thesis topic position}
	\label{fig:venn}
\end{figure}


\section{Mobile Human Computer Interaction}
The term "Human Computer Interaction" involves the study, planning and design of the interaction between human and computers \cite{card_psychology_1983}. This term supports a view both from the computer and from the human perspective. From a computer perspective in the mobile HCI community challenges like short battery life, network volatility, limited memory/processing power typically arise and specific system design patterns have been designed to handle this \cite{roth_patterns_2002}. From a mobile human perspective the term "nomadic computing" can be used where the main requirements for such a system is defined as providing capabilities and services to the nomad as he/she moves from place to place in a transparent, integrated and convenient way \cite{sawhney_nomadic_2000}.

% this project
In this project a mobile music player for cyclists is developed so the focus will be on the interaction between a user and device while the user is in motion. This kind of interaction has also been defined as "interaction in motion" \cite{marshall_mobile_2013}.

\subsection{Interaction in motion}
\label{sec:interactioninmotion}
According to Marshall and Tennent \cite{marshall_mobile_2013} mobile interaction does not exist as most mobile systems are designed for active interaction when a user is standing still dedicating his/her full attention to the device. Specific mobile systems should instead allow the user to interact while in motion e.g. driving, running or in this thesis case biking. To develop such kind of system specific challenges needs to be solved and Marshall and Tennent have classified these into four categories \cite{marshall_mobile_2013}:

\begin{description}
\item[Cognitive Load]
Cognitive Load Theory (CLT) has to do with managing a humans working memory (or short-term memory). It is also concerned with learning complex cognitive tasks. A learner can quickly become overwhelmed (the working memory is limited in capacity) by the number of information elements and their interactions that needs to be processed simultaneously. This can cause loss of information i.e. learning is not meaningful. On a higher level this could mean that a person is only able to pay attention to a certain amount of things at once. When the working memory limit is reached the person stops paying attention to other things. So even though a person is able physically to sense, hear or feel multiple interaction tasks, it may not be possible to attend to all these tasks at the same time. Controlling this cognitive high load has become the main focus of CLT \cite{paas_cognitive_2004}.

% Types of cognitive load
Paas et. al \cite{paas_cognitive_2004} distinguishes between three types of cognitive load; \textit{intrinsic}, \textit{extraneous} and \textit{germane}. The load is called intrinsic if it is imposed of the number of information elements and their interactivity. E.g. making a phone call while solving a puzzle contains two interactivities. If it is imposed by the manner in which the information is presented to the user or by the learning activities required of the user it is called extranous or germane. E.g. when making phone calls a discussion with an interviewer is less distracting than performing a simple memory test \cite{nunes_cognitive_2002}. The difference between extranous and germane load is expressed in their effectiveness meaning how the load is helpful in building schemas and automation so that the next time the user is presented with the same task, it becomes easier to implement i.e. the cognitive load is lower. Here the extranous load is in-effective while the germane load is effective.

% from the systems point of view
From a mobile interactive systems point of view the cognitive load could depend on environmental factors or the movement activity in which the user is performing e.g. walking a plane ground vs. climbing stairs (extranous/germane load). And if the person at the same time is forced to actively attend or respond to the system e.g. answering an important phone call, this would add another activity/interaction to the users working memory and possibly increase cognitive load (intrinsic load).

\item[Physical Constraints]
Mobile systems and user movement activities can both add constraints on the body position which could lead to a conflict making it hard or even impossible to interact. E.g. biking physically needs the legs for cycling, hands for steering and eyes on the road while both of the last mentioned body parts are used in the traditional smartphone interaction form (eyes on the screen, hands for touch gestures). This has been identified to be a major barrier to the use of mobile system whilst moving \cite{pielot_pocketmenu:_2012}.

\item[Terrain]
The terrain is described by the enviroment around a person while moving and interacting. Studies have shown that while running and interacting the terrain over which a person was running made a big difference in the interaction experience and the ability to concentrate on the output of the system \cite{marshall_using_2011}.

Physical terrain can be dynamic i.e. it can change over time while moving around different environments e.g. road obsticles, traffic, light level, rain, sound, etc. This could not only affect the user interaction experience but also the electronic device e.g. water or extreme cold/heat.

\item[Other people]
This class relates to other people during an interaction. For example in a crowded place the user needs to take care of people passing by while interacting with the mobile device. Or when biking the user needs to take care of people making an overhaul, communicating or waving back to a friend on the sidewalk. The social aspect of the environment a person is in can also have an impact on the device interaction e.g. in a quiet place a person would probably not use speech commands to interact with a device as this would be socially impolite.
\end{description}


\section{Multimodal interaction}
While the previous section was about planning and designing the interaction between humans and computers, this section describes how the actual information between a human and computer could be exchanged. 

\subsection{Modality definition and use}
Bolt was one of the first to define the term modality in a study where speech and gesture in combination was used as input to a system \cite{bolt_put-that-there:_1980}. Modalities has something to do with the mode of communication according to the human senses and input devices activated by humans \cite{jaimes_multimodal_2007,tzovaras_dimitrios_multimodal_2008}. The human senses are \textit{sight}, \textit{touch}, \textit{hearing}, \textit{smell} and \textit{taste}. Input modalities of many computer input devices can then be considered to correspond to these senses e.g. cameras (sight), haptic sensors (touch), microphones (hearing). While the term multimodal has been used in many different contexts and disciplines this thesis will focus on Tzovaras definition: \textit{"During interaction, the user produces input modalities to the system and the system produces output modalities to the user. A multimodal interactive system is a system that uses at least two different modalities for input and/or output. And a unimodal system is a system which uses the same single modality for input and output"} \cite{tzovaras_dimitrios_multimodal_2008}. The word input is defined by \cite{jaimes_multimodal_2007} to be of great importance as in practice most interactions take place using multiple modalities e.g. typing a keyboard (touch) while looking at the keys or screen (sight) to see whats being typed.

% modality combinations
Modalities can be used in combination with each other in different ways. In the "Put that there" system \cite{bolt_put-that-there:_1980} modalities are used in a complementary combination as the user can point the items on a large display and select or move them by vocal commands. In this case gestures and speech modalities are strengthening each other. In other cases modalities can act simultaneously giving different kind of information about the same feature e.g. visual and acoustic alarms in a building. Both these modality combinations have no unexpected implications in terms of the information that needs to be delivered. Either the information can only be gained in one way (complimentary approach) or the user can choose which way to gain the information (simultaneous approach). A case where modality combinations can have unexpected behaviour is when a modality is replaced with another to deliver the exact same information e.g. gaining an overview of a music collection would seem unnatural with voice commands compaired to a visual overview. This causes a non-linear effect and introduce more complexity to the interaction system.

\subsection{Multimodality in mobile systems}
As mentioned in \ref{sec:interactioninmotion} interacting with a device in motion introduces different challenges like environmental and human attention factors. This could imply that for some mobile situations certain multimodal interaction types would fit better than others. Much of the interfaces work especially in wearable computing tends to focus on visual headmounted displays \cite{barfield_fundamentals_2000} e.g. Google Project Glass. But not only does visual displays occupy the users visual attention, they can also be obtrusive and hard to use in bright daylight \cite{geelhoed_safety_2000}. Visual displays power consumption is must often also high i.e. they drain a mobile device battery and they are expensive.

% Eyes-free interfaces term
Several work on both audio \cite{kajastila_eyes-free_2013,bonner_no-look_2010,brewster_multimodaleyes-freeinteraction_2003,zhao_earpod:_2007,vazquez-alvarez_eyes-free_2011} and haptic \cite{pasquero_haptic_2011,pielot_tactile_2011} interfaces use the term eyes-free which refers to controlling the state of a system without visual attention. This kind of interaction has shown to be desirable in some mobile situations \cite{oakley_designing_2007,yi_exploring_2012} and even improve efficiency compaired to traditional visual displays \cite{zhao_earpod:_2007}. Eyes-free interfaces can keep the users visual attention on the road while driving \cite{sodnik_user_2008} or walking around in the city \cite{vazquez-alvarez_eyes-free_2011}. It should be taken into account though that just because information comes from a different modality that the one in use, it doesn't mean that the user is not distracted cognitively as described in \ref{sec:interactioninmotion}.

% Eyes-free modalities focus
Eyes-free interfaces are not limited to specific modalities. As mentioned in \ref{sec:alternativemusicuis} input modalities like speech, gesture and touch combined with audio or haptic feedback can "detach" the eyes from the interface. As this project focus is on a concrete mobile scenario i.e. biking - not only should the interface be eyes-free but also "hands-free" (as biking requires steering). To avoid the use of \textit{sight} and \textit{touch} human senses the proposed interface includes head gestures as input and audio as output.

\subsection{Head gestures as input modality}

% intro - alternative head tracking methods
There has been little use of head gestures for input when on the move \cite{brewster_multimodaleyes-freeinteraction_2003} although such gestures are advantageous as a person do not need to look at a display to interact.

There exists different kinds of approaches when it comes to controlling a system with head gestures. Using cameras it is possible to effectively track head movements via facial recognition \cite{morimoto_recognition_1996} and gaze tracking makes it possible to control an object by fixating the eyes on that object while moving the head \cite{vspakov_enhanced_2012}. Thus these techniques do not require any hardware sensors e.g. accelerometer and gyroscope but in return a camera placed in front of the user. Although this method has been conducted for mobile devices \cite{mardanbegi_eye-based_2012} the setup will still require the eyes in a combination with head gestures as an input modality.

Sensor based detection of head movements could also be used e.g. accelerometer and gyroscope for head rotation. This would require some kind of headmounted interface with sensors installed. Both Brewster et. al \cite{brewster_multimodaleyes-freeinteraction_2003} and Park et. al \cite{park_gaze-directed_2011} have evaluated systems using this kind of sensor based head gesture input. Their systems will be described in \ref{sec:auditorygesturebasedmenus}.

\subsection{Audio as output modality}
\label{sec:audiomodality}
Replacing visual with audio output has shown to have a positive effect when interacting in motion. Brewster showed, by compairing visual and audio feedback when pushing buttons on the same GUI, that it was difficult for users to devote all their visual attention to an interface while walking, running og driving and that the interaction workload decreased with audio feedback \cite{brewster_overcoming_2002}.

% Speech vs Non-speech audio
Audio output can in general be divided into two categories \cite{rocchesso_sounding_2003}:
\begin{description}
\item{1: \textit{Speech audio}}, can use a computer recorded human voice like in a guided audio tour for tourists.
\item{2: \textit{Non-speech audio}}, can be used for presenting more complex information e.g. music or other sounds.
\end{description}

The chosen audio output depends on the kind of information that is required by the system e.g. presenting a news feed would be clumsy with non-speech sound. However non-speech audio have advantages over speech audio in that they are faster and language independent. Several studies have shown non-speech audio to be effective when used for interaction in mobile environments although some of the systems uses a combination of both non-speech and speech audio \cite{pirhonen_gestural_2002, sawhney_nomadic_2000, brewster_using_2000}.

When presenting the audio it's important to consider the attention which is required by the user while this can have a great impact on the cognitive load. Studies \cite{vazquez-alvarez_eyes-free_2011, shinn-cunningham_selective_2004} discusses to kinds of attention-tasks:

\begin{description}
\item{1: \textit{Selective-attention task}}, presenting multiple audio streams the user selectively choose which one to assign the attention e.g. listening to a voicemail while listening to a music track. This results in a lower cognitive load.

\item{2: \textit{Divided-attention task}}, in this case the user is forced to respond to each audio stream presented e.g. talking over the phone while interacting with a calendar using a audio menu. The attention gets divided and results in a higher cognitive load.
\end{description}

The user required attention can be difficult to control. Specific user needs and context can constrain the choice of audio representation e.g. a phone call entering might have to stop all other audio output to prevent a possible divided attention. Also it is not always possible for the user to selectively choose which audio channel to devote the attention against - the audio representations needs to be distinguishable. In the case where they are distinguishable the user selectively choose an audio channel and pushes the other in the background and this cognitive phenomenon is called the "Cocktail Party Effect" \cite{bronkhorst_cocktail_2000}. The number of audio channels is limited however - experiments show that increasing the number of channels beyond three affects the cognitive load (overload) \cite{bronkhorst_cocktail_2000}.

\subsection{Spatial audio}
% intro - what is spatial audio, HRTF
Spatial audio or 3D audio is constructed using binaural recordings, HRTF (Head Related Transfer Function), FIR (Finite Impulse Response) filters and other advanced signal processing techniques \cite{begault_3dd_1994}. The word "binaural" literally just means "using both ears" and binaural recordings are reproductions of sound the way the human ear hears it. The sounds are recorded by placing microhones in both ears of a dummy head or a real person. These recordings are compared with the original sounds to compute a persons head-related transfer function. HRTF technology uses advanced signal processing techniques and takes into account many of the cues humans use to localize sound. It uses pairs of FIR filters, one filter for the left ear and one for the right, from specific sound positions. Then, to place a sound in a specific position in a persons virtual space, the filter pair which correspond to the position is applied to the incoming sound. The sound then appear as if it is coming from anywhere in space around a listener and this is also called spatial audio. In short this is an attempt to present sound as we normally hear it in the real world. The technology is implemented in many soundcards today.

% Multiple sound sources, cognitive load effect
Spatialisation of the audio can be helpful in segregating multiple audio streams taking advantage of the "Cocktail Party Effect". Different models of spatialisation can overcome the limit of three simultanously audio streams mentioned in \ref{sec:audiomodality} by allowing listeners to easily switch between channels and pull an audio stream into focus, as well as by allowing sufficient time to fully fuse the audio streams \cite{bronkhorst_cocktail_2000}. Brewster et. al \cite{vazquez-alvarez_eyes-free_2011} shows in a study that when users must be able to direct their attention selectively to each individual non-speech audio stream representing a task (selective-attention task) - spatial audio had a great effect. In this case the cognitive load was kept low. However same study showed that in a divided-attention task (higher cognitive load) with speech audio streams, spatial audio did not have the same effect, on the contrary cognitive load was further increased.

% Advantages refs
Spatial audio has also shown to be effective in other several aspects. William W. Gaver, a pioneer in audio interfaces, has explored the intuitiveness of presenting complex information to users in the form of audio \cite{gaver_sonicfinder:_1989}. Similarly Graham explores the advantages in reaction time when using "auditory icons" \cite{graham_use_1999}. Gaver presents the use of spatial sound icons \cite{gaver_auditory_1986}. In doing so, he draws forward the unutilized potential of creating natural interaction (hearing sounds and navigating through them as we would normally do in the real world) through spatial audio.

TODO: egocentric vs exocentric audiospace


\section{Mobile Audio User Interfaces}
\label{sec:mobileaudiointerfaces}
This section describes the systems that are most related to this project. First the current state on mobile music player interfaces are presented followed by alternative music interfaces - alternative in the sense that the interaction form is different from the traditional way (touch input and visual output). Also other systems - not music related - are presented. These systems uses an interaction form that is close to the desired one in this project. Finally all systems properties are compaired and visualised in a table.

\subsection{Music player interfaces}
\label{sec:alternativemusicuis}
% intro, current state on mobile music players
The rise of the smartphone and the introduction of music streaming services e.g. Spotify\footnote{\url{https://www.spotify.com}} and Deezer\footnote{\url{https://www.deezer.com/}} and more have resulted in an increase in mobile music listening - in fact the number of mobile music listeners has more than doubled from 30 million active users in 2011 to over 70 million in 2013 according to a study in U.S.\footnote{\url{http://www.emarketer.com/Article/Music-Goes-Mobile-More-Smartphone-Users-Stream-Songs/1010126}}. Although mobile audio players have developed since the first portable cassette player by Sony in 1979 towards todays digital audio players with storage capacity e.g. smartphones, a similarity still remains - the way in which we interact with the device. E.g. like we needed the hands and eyes for rewinding or turning the tape on a cassette player, we need them as well for swiping to the preferred track in todays "typical" smartphone music application.

Todays mobile music player interfaces provides a rich list of navigational features. Taking the Spotify application as an example there exists multiple ways for a user to explore saved and also new music e.g. exploring by an artist, album, related songs, friends songs, music charts, genre radio streams and the list goes on. These interfaces however depends heavily on a users visual attention.

Imagine a biking scenario where the user wants to change music track in a smartphone music application. This would require: Fetching the device e.g. from a pocket, switching the screen on, maybe unlocking the screen with a password, navigating through the music application GUI, switch the screen of, put smartphone back in pocket. Also the smartphone touch interface requires bare fingers for input recognition so removing gloves (in winter season) could also be a step in the process. A process that happens while the user is navigating through traffic. Studies show that while a user is doing an activity a vital factor is to minimize the amount of distraction for interaction modes \cite{pascoe_using_2000}. There exists however several music player interfaces that attempts to use eyes-free interaction targeting the interaction while in motion scenario.

\textbf{Headset controllers}

Headset music controllers are physical buttons attached to the wire between headset and device (see figure \ref{fig:nokia}). This controller typically includes limited controls like play, stop, next track, previous track and volume. Such a controller would provide eyes-free interaction and requires only one hand from the user to interact with a music application. These controller interfaces are typically designed for specific smartphone models but can work with all mobile specific music applications as long as their system is designed to handle the control events.

\begin{figure}[t]
	\centering
		\includegraphics[width=0.6\textwidth,height=\textheight,keepaspectratio]{./Figures/nokia-headset.png}
		\rule{35em}{0.5pt}
	\caption[Nokia music headset]{A head music controller from Nokia \cite{nokia_launch:_2011}}
	\label{fig:nokia}
\end{figure}

\textbf{Voice recognition}

Voice recognition could also be used for navigating a music application. Google provides an advanced search system for the Android platform where it is possible to control application features with voice commands\footnote{\url{http://www.google.com/mobile/search/}}. This system provides voice controls for music applications as well. These controls are still quite limited but it is possible to search for a specific artist e.g. speaking "play Coldplay" will play a random Coldplay song. On the contrary the system does not support simple commands like "next song" or "stop".

\textbf{Touch based}

% 1, Gestural and Audio Metaphors As a Means of Control for Mobile Devices
Pirhonen et. al \cite{pirhonen_gestural_2002} developed and evaluated a mobile music player that used touch gestures and non-speech spatial audio to play/stop track, switch track and control volume. The device was an old pocket PC clipped to the users belt (device shown in figure \ref{fig:pirhonen}).

\begin{figure}[t]
	\centering
		\includegraphics[width=0.6\textwidth,height=\textheight,keepaspectratio]{./Figures/pirhonen-system.png}
		\rule{35em}{0.5pt}
	\caption[Pirhonen system]{Figures from Pirhonen et. al \cite{pirhonen_gestural_2002} system showing the touch device (to the left) and touch menu (to the right)}
	\label{fig:pirhonen}
\end{figure}

The menu were designed so that swiping a finger in different directions on the touch screen resulted in an action e.g. changing to next track by swiping to the right. Non-speech audio was used to give feedback on swipe gestures - a sound increased in pitch when swiping to the right (next track) and a decreased sound when swiping left (previous track). The sounds used a simulated (no HRTF) spatial effect - panning the next-track sound to the right and the previous-track sound to the left of the users audio space. While these sounds popup the current chosen track were playing in the center. Users perceived the menu through audio feedback as shown in figure \ref{fig:pirhonen} which almost matched the actual menu design. An evaluation of the system showed significant usability improvements using gestures- and audio based interaction compaired to a visual pen-based interaction approach.

The PocketMenu \cite{pielot_pocketmenu:_2012} also used touch based input via touch screen to control a music player but as output they experimented with two modalities (simultanous approach) - speech audio and tactile feedback. The menu were supposed to be controlled from a users pocket hence the name and were targeting interaction while moving scenarios. The menu layout was designed so that items were positioned vertically along the border of the screen. To browse the menu the user had to slide a finger along the screen. Tactile feedback was given at borders between menu items. Also speech audio was used to indicate which item was currently in focus (speaking the menu item action). By lifting the finger navigation was cancelled and swiping to the right would select the focused item. An illustration of this is shown in figure \ref{fig:pocketmenu}. Menu controls were volume, next/previous track and play/stop. In an evalution of the system the PocketMenu outperformed Apples iPhone VoiceOver system when interacting on the move.

\begin{figure}[t]
	\centering
		\includegraphics[width=0.7\textwidth,height=\textheight,keepaspectratio]{./Figures/pocketmenu.png}
		\rule{35em}{0.5pt}
	\caption[Pirhonen system]{Figure borrowed from the PocketMenu system \cite{pielot_pocketmenu:_2012} showing their menu layout and browsing/selecting model}
	\label{fig:pocketmenu}
\end{figure}

\textbf{Motion gesture based}

Research shows other alternative interfaces that have evaluated motion sensors to control a music player. One system used foot gestures to change music tracks \cite{smus_running_2010}. The system were designed for people that are running while listening to music. With a sensor (Wii remote) attached to the "controlling" leg of the user, it was possible to change track by skipping once on that leg (making two one-footed jumps instead of one full stride). The study showed that this was a highly efficient way of interacting while running.

Another system \cite{strachan_bodyspace_2007} used device tilting based on specific positions on the body to control a music player. More precisely by moving the device to the left ear, tilting would change music track. By moving the device to the left hip, tilting would control the volume of the music player.

Both motion gesture based music player systems are illustrated in figure \ref{fig:bodyandleg}.

\begin{figure}[t]
	\centering
		\includegraphics[width=0.8\textwidth,height=\textheight,keepaspectratio]{./Figures/bodyposeandleg.png}
		\rule{35em}{0.5pt}
	\caption[Alternative music players]{Figures from alternative music interfaces; Body Pose system \cite{strachan_bodyspace_2007} to the left and leg gesture system \cite{smus_running_2010} to the right}
	\label{fig:bodyandleg}
\end{figure}

\subsection{Auditory gesture-based menus}
\label{sec:auditorygesturebasedmenus}
Other systems - though not all mobile music players - have evaluated gesture based input modalities and audio as output modality and they can contribute to this projects final system in form of e.g. menu and audio design, navigation, etc.

% 2, Interaction with eyes-free and gestural interfaces
Kajastila and Lokki \cite{kajastila_eyes-free_2013} compaired auditory and visual menus - in this case they experimented with an input modality that was based on free-hand gestures. The free-hand gestures was detected by a camera and by moving the hand round in a circle containing 12 sound items (one for each 30 degrees) and selecting items were done by simulating a push of a button at the desired item. When moving the hand round the sound items would play in the users audio space (using spatial audio) from the same direction as the hand position in the circle. The sound items were provided as speech-audio i.e. speech recorded numbers. An illustration is shown in figure \ref{fig:kajastila}.

\begin{figure}[t]
	\centering
		\includegraphics[width=0.8\textwidth,height=\textheight,keepaspectratio]{./Figures/kajastila-system.png}
		\rule{35em}{0.5pt}
	\caption[Kajastila system]{Figures borrowed from Kajastila and Lokki \cite{kajastila_eyes-free_2013} system showing the free-hand gestures with spatial audio feedback illustration to the left and visual output to the right.}
	\label{fig:kajastila}
\end{figure}

The system was evaluated by users selecting specific numbers with free-hand gestures with both the auditory menu feedback described above and also a visual based one. The majority of the participants felt that an auditory circular menu was faster than a visual based menu.

% 3, Multimodal'eyes-free'interaction techniques for wearable devices
Brewster et. al \cite{brewster_multimodaleyes-freeinteraction_2003} developed and evaluated an eyes-free wearable system that uses head gestures as input modality and 3D audio as output modality. The systems hardware setup consists of headphones and a sensor attached for detecting the head orientation. These two hardware parts are connected via cables to a belt-mounted PDA running software that processes 3D audio and recognizes head gestures. The physical system is shown in figure \ref{fig:brewster}.

\begin{figure}[b]
	\centering
		\includegraphics[width=0.9\textwidth,height=\textheight,keepaspectratio]{./Figures/brewster-system.png}
		\rule{35em}{0.5pt}
	\caption[Brewster system]{Figures from Brewster et. al \cite{brewster_multimodaleyes-freeinteraction_2003} system showing hardware setup (to the left) and interaction illustration (to the right).}
	\label{fig:brewster}
\end{figure}

By experimenting with the spatial audio presentations two auditory menu design were evaluated. A circular egocentric design illustrated in figure \ref{fig:brewster} and a horizontal exocentric menu design. In the egocentric design audio sources were represented by 4 different audio streams positioned at the 4 cardinal points around the user (every 90 degrees from the users nose). The sound items played for 2 seconds each and play in order rotating clockwise around the head. The audio streams presented different kind of information: 1) Weather sounds e.g. rain, lightening, etc., 2) a news clip from tv, 3) a sport quiz and 4) traffical sounds e.g. cars, horns, etc. The user could then nod at a specific task to select it and it would appear in the front of the users audio space while the other sounds would go to the rear of the audio space. In the exocentric design the same 4 audio streams were represented in a horizontal line with a 40 degree distance between each audio item. The user could then select an audio item by turning the head and nodding when the desired item were in the front.

Evaluation of the system showed that sound and gesture can significantly improve the usability of a wearable device in particular under eyes-free mobile conditions and that head gestures was a successful interaction technique with the circular egocentric soundscape design the most effective.

% 4, Gaze-Directed Hands-Free Interface for Mobile Interaction
Park et. al \cite{park_gaze-directed_2011} also experimented with head gesture input and audio output (though not spatial audio) but in this project focus was on the menu design and its suitability for head gesture movement. The hardware setup were custom built like the aboved mentioned system as a sensor is attached in this case to a cap for detecting head movement combined with headphones illustrated in figure \ref{fig:park}.

\begin{figure}[t]
	\centering
		\includegraphics[width=0.9\textwidth,height=\textheight,keepaspectratio]{./Figures/park-system.png}
		\rule{35em}{0.5pt}
	\caption[Park system]{Figures from Park et. al \cite{park_gaze-directed_2011} system showing hardware setup (to the left) and interaction illustration (to the right)}
	\label{fig:park}
\end{figure}

The interaction design consists of different menus; a 1D horizontal/vertical, 2D grid and 2D circular menu as illustrated in figure \ref{fig:park-menus}. The 1D menus were tested with up to 8 sound items and 2D menus up to 12. Items were speech sounds i.e. speech recorded numbers and played when the users head were directed towards the item. Each menu were coupled with different head movements: The 1D menus were simple 2-way movements, the 2D grid had 4-way and the 2D circular menu had a multi-way wheel i.e. 360 directed selection (also shown in figure \ref{fig:park-menus}). A selection of an item could be done by doing a "long stare" at the desired item - the 2D menu items could also be selected with a "round trip" i.e. directing the head at the current item (in the center) and then towards the desired item and back again.

\begin{figure}[t]
	\centering
		\includegraphics[width=0.9\textwidth,height=\textheight,keepaspectratio]{./Figures/park-menus.png}
		\rule{35em}{0.5pt}
	\caption[Park menus]{Figure from Park et. al \cite{park_gaze-directed_2011} system showing different menus (to the left) and gesture models (to the right)}
	\label{fig:park-menus}
\end{figure}

Evaluation of the system showed that the 2D grid menu was the most effective and had the lowest selection errors.

\subsection{Systems properties overview}
Table \ref{tab:related} shows a comparison between the related systems mentioned above. They outline the current state on systems that are closest to what this thesis aims for. The properties are based on the related systems and research concepts:

\begin{description}
\item[In-motion interaction]
The system is designed for interaction in motion.

\item[Head gestures]
The system uses sensor based head gestures as input modality.

\item[Spatial audio]
The system gives feedback in form of spatial audio. Does not mean that all audio is spatialised.

\item[Exploration of content]
The system gives the opportunity to explore the content before selecting it.

\item[Simultanous audio]
The system is exploiting the "Cocktail party effect" and can present simultanous audio streams. This is depending on whether the system supports spatial audio as it is hard to conduct otherwise.

\item[Hierarchical menu]
The menu has a depth - at least 1 sublevel.

\item[Music Application]
System is designed as a music player interface.
\end{description}

\begin{table}[b] 
\scriptsize
\caption{Related systems properties comparison} % title name of the table 
%\centering % centering table
\begin{tabular}{L{2.2cm}C{1.5cm}C{1.3cm}C{1.3cm}C{1.5cm}C{1.5cm}C{1.5cm}C{1.5cm}} \toprule
	Systems & In-motion interaction & Head gestures & Spatial audio & Exploration of content & Simultanous audio & Hierarchical menu & Music application \\ \midrule
    %Properties & System 1 & System 2 & System 3 & System 4 \\ \midrule
    Current "stop-to-interact" systems   &  &  &  & x &  & x & x \\
    \\
    Headset controllers   & x &  &  &  &  &  & x \\
    \\
    Voice recognition system (Google Search for Android)   & x &  &  &  &  &  & x \\
    \\
	Leg gesture system \cite{smus_running_2010}   & x &  &  &  &  &  & x \\ %\midrule
	\\
	Body pose system \cite{strachan_bodyspace_2007}   &  &   &  &  &  &  & x \\
	\\
	The PocketMenu \cite{pielot_pocketmenu:_2012}   & x &  &  &  &  &  & x \\
	\\
	Touch gesture system \cite{pirhonen_gestural_2002}   & x &  & x &  &  &  & x \\
	\\
	Free-hand gesture system \cite{kajastila_interaction_2013}   &  &  & x & x &  &  &  \\
	\\
	Head gesture system, Park et. al \cite{park_gaze-directed_2011}   & x & x & x & x &  &  &  \\
	\\
	Head gesture system, Brewster et. al \cite{brewster_multimodaleyes-freeinteraction_2003}   & x & x & x & x & x &  &   \\
	\\
	Goals of this thesis system   & x & x & x & (x) & (x) & (x) & x \\ \bottomrule
\end{tabular}

\label{tab:related} 
\end{table}

We see that no system supports all properties. The mark (x) specified at this thesis system goals means that although not a required property, maybe related work has shown good results by using these concepts e.g. the Brewster \cite{brewster_multimodaleyes-freeinteraction_2003} system using dynamic auditory menu content and simultanous audio, or else just a desired property e.g. hierachical menu.





% OLD

%All these systems have in common that they avoid the users visual attention but they are all limited to basic controls such as play, stop, next/previous track, volume up/down.

%There exists advanced algorithms for recognizing motion gestures \cite{lu_head_2005, kratz_combining_2013, akl_accelerometer-based_2010}.

%Simultaneous Listening. It is possible for listeners to attend to multiple background processes via the auditory channel as long as the sounds representing each process are distinguishable. This well known cognitive phenomenon, called the "Cocktail Party Effect" [Arons 1992], provides the justification that humans can in fact monitor several audio streams simultaneously, selectively focusing on any one and placing the rest in the background. A good model of the head-related transfer functions (HRTF) permits effective localization and externalization of sound sources [Wenzel 1992]. However experiments show that increasing the number of channels beyond three causes an increase in cognitive overload and hence a degradation in comprehension. Arons [1992] suggests that the effect of spatialization can be improved by allowing listeners to easily switch between channels and pull an audio stream into focus, as well as by allowing sufficient time to fully fuse the audio streams. We will demonstrate how such techniques can be used for browsing and rapidly scanning audio messages.

%Previous research has also shown that spatial audio is a successful technique for segregating multiple audio streams \cite{schmandt_audiostreamer:_1995, walker_spatial_2000}.  as described in \ref{sec:audiomodality}.

%with multiple audio streams, spatial sound can increase cognitive load if used improperly \cite{vazquez-alvarez_eyes-free_2011}


