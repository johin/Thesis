\lhead{\emph{Introduction}}
\chapter{Introduction}
\section{HCI in mobile environments}
Mobile and wearable devices has been a growing area in computing in recent years. Compaired to desktop computers these devices have introduced new standards for when and how people interact with especially mobile applications. Suddenly people are able to check the news, navigate via interactive maps, post social messages, listen to music, etc., while they are on the move. At the same time emerging hardware e.g. sensor technology in mobile devices and wearable computing expands the user interaction possibilities.

Challenges arise when interacting with mobile devices. Although screen resolutions and physical sizes of mobile devices are increasing, the visual work space is limited i.e. screens easily becomes cluttered with information and the input keyboard can be challenging when moving e.g. small buttons or non-responding touch interfaces. More importantly, when moving around e.g. in the traffic, interacting with a mobile device at the same time can create challenges in form of distractions e.g. "eyes off the road" or "hands occupied" and in the worst case cause accidents. Motivated by this problem fines are introduced (in Denmark) for people interacting with their mobile device while biking \cite{cyklistforbundet_bodetakster_2014}. Fines are also being introduced in the U.S. for texting while biking e.g. in California \cite{_there_2011} and Charleston \cite{beahm/staff_charleston_2013}. As Charleston law suggests there are exceptions: \textit{"The exception would be for a device that can be worked hands-free."}.

So it seems that solutions to this problem could be found in the area of "interacting while in motion". The emerging hardware (e.g. sensor technology) and software opens up for alternative input modalities e.g. head gestures, gaze tracking, speech recognition making hands-free interaction possible. At the same time output modalities such as audio and haptic feedback could liberate the eyes from the screen.

\section{Problem statement}
Considering interacting with a mobile device while in motion, this project will be based on the concrete scenario where people are biking while listening to and controlling their music libray. As biking requires eyes on the road and hands for steering the input/output modalities should preferrably not include eyes and hands. Instead head gestures for input and 3D audio for output will be evaluated.

More specifically the following questions should be answered. Can a user interface based on head gestures and 3D audio compete with existing user interfaces for music players (e.g. touch and vision-based) with respect to for instance:
\begin{description}
\item[1] Navigating content (exploring music tracks)
\item[2] General usability (cognitive/perceptive load)
\item[3] Suitability to real-world hands-occupied situations
\end{description}
Furthermore could this alternative interaction mode with the chosen input and output modalities improve the safety when biking in a traffic environment?

%TODO: Something about exploring the music content? Compaired to not just switching track (next/prev button)

% OLD
%More specifically the following questions should be answered. Can a user interface based on head gestures and 3D audio compete with existing user interfaces for music players (e.g. touch and vision-based) with respect to for instance:
%\begin{description}
%\item[1] Navigation and control efficiency
%\item[2] Learnability
%\item[3] General usability (cognitive/perceptive load)
%\item[4] Suitability to real-world hands-occupied situations
%\end{description}
%With the chosen combination of input and output modalities, there is a high risk for the system to misinterpret normal everyday actions performed by the user as commands for controlling the system ("behavioural cluttering" (Janlert et al., in press)). How can features in the user interface prevent unwanted manipulation of the system?

\section{Goal}
To measure properties from the problem statement the goal of this thesis will be to:
\begin{itemize}
\item Design an interface that can detect head movements and provide audio feedback and at the same time is appropriate in the concrete mobile scenario where people are biking.
\item Design and implement music player software that can handle data from the interface and present this in form of a user friendly navigable menu.
\item Imperically compare the new music player with an existing one and gather information on whether the developed system possibly can increase safety when biking in traffic i.e. make people more aware of what is going on around them while they are biking and navigating the system.
\end{itemize}

% OLD
%The goal of this project is to examine if head gesture based input and audio output modalities in combination can compete with a traditional touch and vision based input/output interface and show which advantages, disadvantages and challenges that arise when designing and using such interaction techniques. More precisely a mobile system that recognises these alternative interaction methods should be designed and implemented in a music application. To measure properties from the problem statement, the final system should contain:
%\begin{description}
%\item[1] Music menu navigation
%\item[2] Head gestures recognition
%\item[3] Menu items in a users 3D audio space
%\end{description}
%Such a system should be evaluated in a scenario where the user interacts while in motion e.g. a biking scenario.


\section{Method}
In short the method is to design, implement and evaluate a system that achieves the goals of this project. The first system prototype is build from chosen specific related system designs. This prototype will then go into an iterative process where test users evaluate the system and based on their feedback new modifications to the design will be implemented and evaluated again etc. This will result in a final prototype that will be the model for a bigger evaluation. The activities during this thesis are shown in figure \ref{fig:triangulation}.

\begin{figure}[htbp]
	\centering
		\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{./Figures/triangulation.pdf}
		\rule{35em}{0.5pt}
	\caption[Triangulation]{Mapping of thesis activities and process using the triangulation framework proposed by Mackay and Fayard \cite{mackay_hci_1997}}
	\label{fig:triangulation}
\end{figure}

TODO: methods from design section

% OLD
%In the final evaluation users will compaire this new way of controlling a music application with a traditional music application in form of usbility, efficiency, learnability and suitability. This will happen in a closed lab test where users should bike while navigating the system.





