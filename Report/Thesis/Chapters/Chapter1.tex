\lhead{\emph{Introduction}}
\chapter{Introduction}
\section{HCI in mobile environments}
Mobile and wearable devices has been a growing area in computing in recent years. Compaired to desktop computers these devices have introduced new standards for when and how people interact with especially mobile applications. Suddenly people are able to check the news, navigate via interactive maps, post social messages, listen to music, etc., while they are on the move. At the same time emerging hardware in mobile devices and wearable computing expands application complexity and interaction possibilities.

This mobility factor introduces challenges when interacting with these devices. Although screen resolutions and physical sizes of mobile devices are increasing, the visual work space is limited i.e. screens easily becomes cluttered with information and the input keyboard can be an interaction challenge when moving. More importantly, when moving around e.g. in the traffic, interacting with a mobile device at the same time can create challenges in form of distractions e.g. "eyes off the road" or "hands occupied" and in the worst case cause accidents. Motivated by this problem fines are introduced (in Denmark) for people interacting with their mobile device while biking \cite{cyklistforbundet_bodetakster_2014}.

Solutions for these distraction challenges could lie in the interaction between users and mobile devices. The emerging hardware (e.g. sensor technology) and software opens up for alternative input modalities e.g. head gestures, gaze tracking, speech recognition making hands-free interaction possible. At the same time output modalities such as audio and haptic feedback could liberate the eyes from the screen.

\section{Problem statement}
Considering distraction challenges in mobile interaction, this project will be based on the concrete scenario where people are biking while listening to and controlling their music libray. As biking requires eyes on the road and hands for steering the input/output modalities should preferrably not include eyes and hands. Instead head gestures for input and 3d audio for output will be evaluated.

More specifically the following questions should be answered. Can a user interface based on head gestures and 3d audio compete with existing user interfaces for music players (e.g. touch and vision-based) with respect to for instance:
\begin{description}
\item[1] Navigation and control efficiency
\item[2] Learnability
\item[3] General usability (cognitive/perceptive load)
\item[4] Suitability to real-world hands-occupied situations
\end{description}
With the chosen combination of input and output modalities, there is a high risk for the system to misinterpret normal everyday actions performed by the user as commands for controlling the system ("behavioural cluttering" (Janlert et al., in press)). How can features in the user interface prevent unwanted manipulation of the system?

\section{Goal}
The goal of this project is to examine if head gesture based input and audio output modalities in combination can compete with a traditional touch and vision based input/output interface and show which advantages, disadvantages and challenges that arise when using and designing such interaction techniques. More precisely a mobile system that recognises these alternative interaction methods should be designed and implemented in a music application. [TODO: Make goals quantifiable if possible]

\section{Method}


\begin{figure}[htbp]
	\centering
		\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{./Figures/triangulation.pdf}
		\rule{35em}{0.5pt}
	\caption[Triangulation]{Mapping of thesis activities and process using the triangulation framework proposed by Mackay and Fayard \cite{mackay_hci_1997}}
	\label{fig:triangulation}
\end{figure}

Test users will compaire this new way of controlling a music application with a traditional music application in form of comfort, efficiency and learnability in a closed lab test and a "real world" scenario.

Project structure description (short)...





