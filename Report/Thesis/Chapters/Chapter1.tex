\lhead{\emph{Introduction}}
\chapter{Introduction}
\section{Mobile HCI}
Mobile and wearable devices has been a growing area in computing in recent years. Compaired to desktop computers these devices have introduced new standards for when and how people interact with especially mobile applications. Suddenly people are able to check the news, navigate via interactive maps, post social messages, listen to music, etc., while they are on the move. At the same time emerging hardware in mobile devices and wearable computing expands application complexity and interaction possibilities.

This mobility factor introduces challenges when interacting with these devices. Although screen resolutions and physical sizes of mobile devices are increasing, the visual work space is limited i.e. screens easily becomes cluttered with information and the input keyboard can be an interaction challenge when moving. More importantly, when moving around e.g. in the traffic, interacting with a mobile device at the same time can create cluttering in form of distractions e.g. "eyes off the road" or "hands occupied" and in the worst case cause accidents.

Solutions for these cluttering challenges could lie in the interaction between users and mobile devices. The emerging hardware (e.g. sensor technology) and software opens up for alternative input modalities e.g. head gestures, gaze tracking, speech recognition making hands-free interaction possible. At the same time output modalities such as audio and haptic feedback could liberate the eyes from the screen.

\section{Problem statement}
Considering mobile interaction cluttering challenges, this project will be based on the concrete scenario where people are biking while listening to and controlling their music libray. As biking requires eyes on the road and hands for steering the input/output modalities should preferrably not include eyes and hands. Instead head gestures for input and 3d audio for output will be evaluated.

More specifically the following questions should be answered:\\
Can a user interface based on head gestures and 3d audio compete with existing user interfaces for music players (e.g. touch and vision-based) with respect to for instance a) navigation and control efficiency b) learnability, c) general usability (cognitive/perceptive load), c) suitability to real-world hands-occupied situations. 

With the chosen combination of input and output modalities, there is a high risk for the system to misinterpret normal everyday actions performed by the user as commands for controlling the system ("behavioural cluttering" (Janlert et al., in press)). How can features in the user interface prevent unwanted manipulation of the system?

\section{Method}
Use triangle framework for HCI design \cite{mackay_hci_1997}

% \section{Project goal}
% The goal of this project is to examine if head gesture based input and audio output modalities in combination can compete with a traditional touch and vision based input/output interface and show which advantages, disadvantages and challenges that arise when using and designing such interaction techniques. More precisely a mobile system that recognises these alternative interaction methods should be designed and implemented in a music application. Test users will compaire this new way of controlling a music application with a traditional music application in form of comfort, efficiency and learnability in a closed lab test and a "real world" scenario.

\section{Project structure}
...

\section{Limitations}
...

\section{Related work}
Venn diagram: Introducing the academic areas...
- Head gesture interaction
- Audio feedback/menu
- Accessible hardware/mobile devices
- Eyes-free focus
- Hands-free focus
- Real-world scenario/application evaluation

Related work: Split up in academic research areas

% Visual attention

% Gesture interaction

% Gesture cluttering

% Wearable computing

% 3D Audio

% (Music as feedback?)

Distractions when mobile/"in-the-field"

Pascoe et al. investigated HCI issues when people are on the move and trials showed that a vital factor was to minimize the amount of distraction for interaction modes \cite{pascoe_using_2000}.

Visual displays can be obtrusive and hard to use in bright daylight, plus they occupy the users’ visual attention \cite{geelhoed_safety_2000}

Kajastila and Lokki has done a user study comparing auditory and visual menus controlled by the same free-hand gestures where the majority of the participants felt that an auditory circular menu was faster than a visual based menu \cite{kajastila_interaction_2013}.

Brewster et al. showed that novel interaction techniques based on sound and gesture can significantly improve the usability of a wearable device in particular under "eyes-free" mobile conditions and that head gestures was a successful interaction technique with egocentric sounds the most effective \cite{brewster_multimodaleyes-freeinteraction_2003}.

William W. Gaver, a pioneer in audio interfaces, has explored several aspects of using sound in interfaces including the intuitiveness of presenting complex information to users in the form of audio \cite{gaver_sonicfinder:_1989}. Similarly Graham explores the advantages in reaction time when using ”auditory icons” \cite{graham_use_1999}. In \cite{gaver_auditory_1986} Gaver presents the use of spatial sound icons. In doing so, he draws forward the unutilized potential of creating natural interaction through spatial audio.

Table: Summing up references that handles specific research areas




