\lhead{\emph{Implementation}}
\chapter{Implementation}
This chapter describes the different components of the Spatial Music Menu system. The main components includes a headset interface and an iOS application controlling the headset.


\section{Headset}
\label{sec:implementationheadset}
The headset interface used in this project is the Intelligent Headset (IHS)\footnote{\url{https://intelligentheadset.com/}}. The headset uses HRTF (Head Related Transfer Functions, described in section \ref{sec:relatedworkspatialaudio}) technology and can deliver spatial audio. 4 sensors are built into the headset; GPS, compass, gyroscope and accelerometer - making it possible to track head rotation and location of the user wearing it. The headset also includes 2 buttons - one in the outer center on each speaker. Connection to the headset is accessible via Bluetooth or wire. The headset is shown in \ref{fig:headset}.

\begin{figure}[h]
	\centering
		\includegraphics[width=0.9\textwidth,height=\textheight,keepaspectratio]{./Figures/headset.jpg}
		\rule{35em}{1pt}
	\caption[The Intelligent Headset]{The Intelligent Headset. A button is placed in the center of the left and right speaker (shown to the right).}
	\label{fig:headset}
\end{figure}

The headset features can be exploited through mobile applications using an included SDK targeting the iOS and Android platform (iOS SDK currently at version 1.82 and an Android SDK running verson 1.21 \footnote{Version information from 05-05-2014. Developer site: \url{https://developer.intelligentheadset.com/our-sdk/}}). The platform used in this project is Apples iOS. The main reason for using this platform is that when this project started the Intelligent Headset only provided an SDK compatible with iOS and still - though accessible for the Android platform today - the iOS SDK is running a higher version number making it more mature i.e. stable.

The SDK is implemented in the Spatial Music Menu application. The Intelligent Headset API (Application Programming Interface) provides features like reading the raw sensor data from all sensors, button events and functions for headset connectivity and 3D audio handling. An overview of the components and their communication is shown in figure \ref{fig:implementationoverview}.

\begin{figure}[t]
	\centering
		\includegraphics[width=0.9\textwidth,height=\textheight,keepaspectratio]{./Figures/implementation_overview.png}
		\rule{35em}{1pt}
	\caption[Implementation overview]{An overview of the components of the implementation and their communication}
	\label{fig:implementationoverview}
\end{figure}

\subsection{3D audio handling}
The combination of realtime sensor data and the IHS SDK functionality of setting specific audio items positions provided the required tools for implementing the audiospace. To place the audio items we used a 3D audio grid that served as a model for the audio listener and audio items, but also as a view mapping this audiospace. This 3D audio grid were defined with a virtual area size e.g. 100x100 meters.

To place audio items in a circular way we used simple geometry. We only needed the horizontal position the x and y positions of audio items were positioned by the parametric equation for a circle:

$x = cx + r * cos(a)$

$y = cy + r * sin(a)$

Where $r$ is the radius, $cx$,$cy$ the origin, and $a$ the angle from $0$ to $2\pi$ radians. Same equation were used for the audio listener position for the "carousel" zoom effect. The listeners direction were simply set by the yaw value from the gyroscope sensor data output which is the rotation value (in degrees) around the z axis. An illustration of this is showed in figure \ref{fig:circlepositions}.

\begin{figure}[t]
	\centering
		\includegraphics[width=0.6\textwidth,height=\textheight,keepaspectratio]{./Figures/circlepositions.png}
		\rule{35em}{1pt}
	\caption[Audio items positioning]{Listener and audio items positioning using circle equations.}
	\label{fig:circlepositions}
\end{figure}


\section{Music}
\label{sec:implementationmusic}
We used Deezer\footnote{\url{https://www.deezer.com/}} as a music provider in the implementation. Deezer is a music streaming service and provides an iOS SDK for accessing different kinds of information through their REST API e.g. track info, user playlists, albums, streaming urls, etc.

The auditory menu items are music track samples with lengths of 30 seconds provided by Deezer. The fact that Deezer provides these downloadable sample clips of all their music was one of the main reason for using their services. This is very convinient as it is not possible to spatialise audio streams using the IHS SDK. Instead the IHS SDK is quite limited in that the only way to place sound sources in the spatial audio space is to use 16-bit wav format. Deezers track samples comes in MP3 format so every track needed conversion to 16-bit wav before use.

A track represents an artist in the first level of the menu (HOME level) and by selecting a track (going up to ALBUM level) only tracks that are from the same album as the chosen track will appear. The final selection is simply the chosen album track (PLAYING TRACK level).

All music information, download of preview tracks and conversion of audio was handled dynamically inside the Spatial Music Menu iOS application making it easier to add, remove and switch tracks. Basically one could add music tracks to the application by creating a Deezer playlist which is then synchronized by the iOS application. A synchronization operation is simply: 1) Getting and saving all track information from playlists; 2) download all MP3 tracks and save on device; 3) convert all tracks to 16-bit wav and save on device.


\section{Head Gestures Recognition}
\label{sec:implementationgesturerecognition}
To recognize head gestures we used DTW (Dynamic Time Warping) which is a well-known technique to find an optimal alignment between two time dependent sequences \cite{muller_dynamic_2007} - in this case sequences of sensor data. Due to the simple gestures (short sequences) defined in the system design we used a classic simple DTW although there exists several techniques for speeding up the algorithm \cite{muller_dynamic_2007,salvador_toward_2007,akl_accelerometer-based_2010}. 

Essentially the DTW algorithm works by compairing two sequences of length N and M. A sequence consists of observations and an observation is a set of accelerometer data from the headset. For each observation comparison a local cost is calculated and a cost matrix is created (using dynamic programming). If there exists a path (warping path) from point (1,1) to (N,M) in the cost matrix that has an overall cost that is less than a predefined cost, then there is a match. The local cost of an observation is calculated from the sum of the euclidean squared distances of each observation value. A squared euclidean distance places progressively greater weight on objects that are farther apart. The total cost is then simply the sum of all the observation distances.

A preview of such a sequence example could look like this:

\begin{lstlisting}
...
Observation: (
    "0.1245155",
    "-0.9570605",
    "0.02343822"
),
Observation: (
    "0.3266701",
    "-1.189489",
    "-0.1518601"
),
Observation: (
    "0.09228797",
    "-1.126988",
    "0.01416059"
)
...
\end{lstlisting}

The values are 3-axis accelerometer data (in order) x, y and z. A sensor observation occurs $\sim$ 30 times per second. To avoid overloading the device CPU by trying to recognise gestures for every new observation we only run the gesture recognizer for every 30 observations i.e. every second. All gesture sequences are below 100 observations so we used a window size of 100 for the testing sequence. I.e. every second the last 100 observations are tested for possible gestures.

\subsection{Training data}
Compaired with other long motion gestures e.g. running or climbing stairs, our head gestures were short events. For obtaining training data these gestures were recorded per event so a simple view containing a start/stop record button was implemented. Also to identify which class the gesture belonged to, the view had a list of labels with the specific gesture type that was to be chosen before recording the specific gesture.

The possible pause between pushing the start/stop recording button and the start or end of the actual gesture would cause some noise in the recorded data. To solve this we removed observations from the start of the recorded sequence until the difference between the start and the following observation were \textgreater 0.01. The same check was done with the end and its preceding observations. This noise cleaning was done after gestures were recorded (in memory) and the cleaned training data was saved to the device storage.


\section{iOS Application}
The Spatial Music Menu appliation were developed for an iPad Mini running iOS 7.1. As an overview and introduction of the iOS framework is out of this thesis scope - the interested reader is referred to Apples developer portal\footnote{\url{https://developer.apple.com/}} where a comprehensive documentation on the framework is available. Instead this section will focus on the application architecture, the design patterns and most the important functionality of the Spatial Music Menu.

An overview of the application architecture is shown in figure \ref{fig:apparchitecture} and this diagram will function as a reference throughout the presentation of the iOS application in this section. The figure highlights the different relationships between application state, music client, headset events handling, view (and audio) controller logic and persistency storage. The application aims to use iOS best practice design patterns provided by Apple, where some of the frequently used patterns are Model-View-Controller and Delegation\footnote{\url{https://developer.apple.com/library/ios/referencelibrary/GettingStarted/RoadMapiOS/DesignPatterns.html}}.

\begin{figure}[t]
  \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{./Figures/app-architecture.pdf}
    \rule{35em}{1pt}
  \caption[App architecture]{Spatial Music Menu iOS app architecture overview}
  \label{fig:apparchitecture}
\end{figure}

\subsection{Application state management}
For handling application state and events we use a singleton approach. Every iOS app have exactly one instance of an application object and this is guaranteed to live (exist in memory) as long as the app is running. This singleton object has an application delegate (AppDelegate class) object where application state transitions are being handled like when an app is launched or closed (put in to the background). This delegate is also used for handling custom application events and we use it to hold exactly one instance of each 3 main classes: PersistencyManager (local storage), DeezerClient (music events) and SMMDeviceManager (headset events). 

\subsection{Models and persistency storage}
\label{sec:modelsandpersistency}
We are saving different kinds of information and objects throughout the application. Every iOS app have a sandbox - an application folder for file writing and reading. We use this folder to save music tracks and also data files containing information like playlists, tracks and recorded gestures (training data). Other simple user preferences like menu settings or headset device information are saved in a user default system which is provided with every iOS app and designed for this in particular.

Tracks, albums and gestures data consists of model objects e.g. Album, Track and Gesture. We used Mantle\footnote{\url{https://github.com/Mantle/Mantle}}, a model layer library, to easy serialize/deserialize the objects into/from binary files. We saved log data in a textfile - a log event was simply the event (as a string) and a timestamp. A music track is saved as both an MP3 and a WAV file due to the needed conversion for spatialisation usage (as mentioned above). This process of downloading, converting and saving a track is handled by a TrackArchiver object and is running in a background queue not blocking the main thread i.e. avoiding the app freezing.

As illustrated in figure \ref{fig:apparchitecture} the PersistencyManager is part of a notifier/observer pattern. Using the iOS NSNotificationCenter it is possible to dispatch and listen to custom events. The PersistencyManager dispatches an event when track data has been changed (TRACK\_DATA\_CHANGED) which is the listened to by objects that needs update like for example the audio menu (AudioMenu ViewController).

An example of using model objects, track archiving and notifications is shown is this example where track data is synchronized for a playlist:

\begin{lstlisting}
- (void)syncTrackDataForPlaylistWithId:(NSString *)itemId
{
  dispatch_queue_t myQueue = dispatch_queue_create("Playlist Queue",NULL);
  dispatch_async(myQueue, ^{
        
    // Perform long running process
    Playlist *pl;
    for (int i=0; i<[_playlists count]; i++) {
        pl = [_playlists objectAtIndex:i];
        if([pl.itemId isEqualToString:itemId])
        {
          // Tracks
          for (int j=0; j<[pl.tracks count]; j++)
          {
            if(j < kTrackLimit)
            {
              Track *track = [pl.tracks objectAtIndex:j];
              TrackArchiver *archiver = [[TrackArchiver alloc] init];
              [archiver archiveTrack:track];
              
              archiver = nil;
            }
          }
        }
    }
    // finished
    dispatch_async(dispatch_get_main_queue(), ^{
        // E.g. update the UI
        [[NSNotificationCenter defaultCenter] 
            postNotificationName:DEEZER_PLAYLIST_DATA_UPDATED 
                          object:self 
                        userInfo:nil];
    });
  });
}
\end{lstlisting}

\subsection{Music client}
The DeezerClient object is handling functionality and events associated with the music content. It uses the Deezer SDK library classes to first of all establish a connection to Deezer and then to make HTTP requests to their API. Acting as a delegate it receives events for Deezer connection state and request responses. The response results are returned as JSON and could look like this (track example):


\begin{lstlisting}
"tracks": {
    "data": [
      {
        "id": "1152226",
        "readable": true,
        "title": "Darkroom",
        "link": "http://www.deezer.com/track/1152226",
        "duration": "271",
        "preview": "http://cdn-preview-6.deezer.com/stream/6452dbcd46c90a70cb1147666ffd91ae-0.mp3",
        "artist": {
          "id": "1334",
          "name": "Archive",
          "link": "http://www.deezer.com/artist/1334",
          "tracklist": "https://api.deezer.com/artist/1334/top?limit=50",
          "type": "artist"
        },
        "album": {
          "id": "123427",
          "title": "Londinium",
          "cover": "https://api.deezer.com/album/123427/image",
          "tracklist": "https://api.deezer.com/album/123427/tracks",
          "type": "album"
        },
        "type": "track"
      },
\end{lstlisting}

Another reason for using the Mantle model layer mentioned in \ref{sec:modelsandpersistency} is that it can serialize JSON into model objects with a few steps. The track data above is serialized into a Track object with the following properties:

\begin{lstlisting}
@interface Track : MTLModel <MTLJSONSerializing>

@property (nonatomic, copy, readonly) NSString *itemId;
@property (nonatomic, copy, readonly) NSString *artist;
@property (nonatomic, copy, readonly) NSString *title;
@property (nonatomic, copy, readonly) NSURL *preview;
@property (nonatomic, copy, readonly) NSString *stream;
@property (nonatomic, copy, readonly) NSString *albumId;
@property (nonatomic, copy, readonly) NSString *albumName;

@end
\end{lstlisting}

The DeezerClient also takes care of music playback for the PLAYING TRACK level in the menu simply using the saved MP3 track sample (downloaded from preview url). Thus the application do not depend on an internet connection for playback - only when synchronizing music.

\subsection{Headset events handling}
All functionality related to head movement is handled by the SMMDeviceManager. This manager is a delegate for the connection, sensor and button events received by the Intelligent Headset (IHSDevice class). The class is wrapping the proper headset events and sensor data readings into a minimum set of relevant events needed by the application. These are events triggered when changes occur in device connection, heading, right button press and gesture recognition. The manager has an instance of the DTWRecognizer object which is the actual DTW algorithm (page \pageref{sec:implementationgesturerecognition}). Other objects can act as delegate for the SMMDeviceManager to receive these events e.g. the AudioMenu ViewController is the main delegate as it implements the actual menu. To receive event information the class simply needs to 1) implement the delegation class, 2) set itself as the delegate and 3) use the proper event method. An example of getting gesture recognition events from the AudioMenu ViewController could look like this:

\begin{lstlisting}
@interface AudioMenuViewController () <SMMDeviceManagerDelegate>

...

APP_DELEGATE.smmDeviceManager.delegate = self;

...

- (void)smmDeviceManager:(SMMDeviceManager *)manager gestureRecognized:(NSDictionary *)result
{
  // E.g. nod = go to sublevel
  ...
}
\end{lstlisting}

\subsection{Views and controller logic}
\label{sec:implementationviewsandcontrollers}
All application feedback is presented in views. Although the auditory menu of the Spatial Music Menu aims to use no visual feedback, the auditory output is still being considered a "view" in an application context i.e. audiovisual feedback. Also the fact that we used envisionment to show the auditory menu on screen makes the auditory menu view both "screen visual" and audiovisual. This is the main view of the application and is referred to as the AudioMenu in the architectural diagram. There exists two other views in the application, which are mainly used for preference settings, and they are the Music view showing music content information and the Gestures view showing gesture related controls e.g. recording gestures button. The view controllers takes care of creating and updating the views and are also handling the interaction logic e.g. 3D audio handling and navigating between menu levels. This code snippet example is from the for AudioMenu view controller class:

\begin{lstlisting}
- (void)changeAudioMenuState:(int)state
{
  _audioMenuState = state;
  
  DEBUGLog(@"Changing state to: %d", _audioMenuState);
  
  switch (state) {
    case MENU_HOME:
    {
      Playlist *pl = [APP_DELEGATE.persistencyManager getActivePlaylist];
      _selectedPlaylistTracks = [APP_DELEGATE.persistencyManager getAlbumdistinctRandomTracksFromPlaylist:pl];
      [self initMenuWithTracks:_selectedPlaylistTracks 
                      AndLimit:APP_DELEGATE.persistencyManager.trackNumber];
      [APP_DELEGATE.deezerClient pausePlayback];
      [_lblState setText:@"Home"];
      [self playSystemSoundWithName:@"home"];
      [APP_DELEGATE.smmDeviceManager playAudio];
      break;
    }
    case MENU_ALBUM:
    {
      ...
    }
    case PLAYING_TRACK:
    {
      ...
    }
    default:
      break;
  }
}
\end{lstlisting}

The code snippet shows selected features of a method that handles the different menu levels - in this case HOME level actions are highlighted.

The views are aligned in a horizontal way with the main view, the AudioMenu, in the center and the Music and Gestures view to the left and right respectively. A simple swipe to on of the sides would reveal the specific view (taking up half of the screen width). Figure \ref{fig:implementationviews} shows the 3 different views. For full size sceenshots the interested reader is referred to appendix \ref{sec:appendixviews}.

\begin{figure}[t]
  \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{./Figures/views_2.png}
    \rule{35em}{1pt}
  \caption[iOS views]{3 different views presents the visual feedback on an iPad. Music view to the left, AudioMenu view in the middle and Gestures view to the right.}
  \label{fig:implementationviews}
\end{figure}

The Music view provides setting features like playlist selection, number of tracks, connecting to the Deezer music service and synchronizing playlist and tracks. The Gestures view provides one funtion and that is recording of gestures. It also shows a list of recorded gestures and highlights the gesture item on the list that is being recognized. The main AudioMenu view provides menu settings like distance, max rotation degree, interaction modes (with or without zoom effect), a home button (takes the user to the HOME level) and a calibrate button which is used for aligning the center of the menu with the users center head direction.

The application in action, with headset and iPad screen showing the visualised auditory menu (AudioMenu view), is illustrated in figure \ref{fig:implementationsystem}.

\begin{figure}[t]
  \centering
    \includegraphics[width=0.6\textwidth,height=\textheight,keepaspectratio]{./Figures/system_final.jpg}
    \rule{35em}{1pt}
  \caption[Final system in action]{The system in action with iOS application running on iPad and the Intelligent Headset.}
  \label{fig:implementationsystem}
\end{figure}







