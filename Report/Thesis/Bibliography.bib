
@article{gaver_auditory_1986,
	title = {Auditory Icons: Using Sound in Computer Interfaces},
	volume = {2},
	issn = {0737-0024},
	shorttitle = {Auditory Icons},
	url = {http://dx.doi.org/10.1207/s15327051hci0202_3},
	doi = {10.1207/s15327051hci0202_3},
	abstract = {There is growing interest in the use of sound to convey information in computer interfaces. The strategies employed thus far have been based on an understanding of sound that leads to either an arbitrary or metaphorical relation between the sounds used and the data to be represented. In this article, an alternative approach to the use of sound in computer interfaces is outlined, one that emphasizes the role of sound in conveying information about the world to the listener. According to this approach, auditory icons, caricatures of naturally occurring sounds, could be used to provide information about sources of data. Auditory icons provide a natural way to represent dimensional data as well as conceptual objects in a computer system. They allow categorization of data into distinct families, using a single sound. Perhaps the most important advantage of this strategy is that it is based on the way people listen to the world in their everyday lives.},
	number = {2},
	urldate = {2014-01-08},
	journal = {Hum.-Comput. Interact.},
	author = {Gaver, William W.},
	month = jun,
	year = {1986},
	pages = {167–177}
},

@inproceedings{calandra_cowme:_2013,
	address = {New York, {NY}, {USA}},
	series = {{ICMI} '13},
	title = {{CoWME:} A General Framework to Evaluate Cognitive Workload During Multimodal Interaction},
	isbn = {978-1-4503-2129-7},
	shorttitle = {{CoWME}},
	url = {http://doi.acm.org/10.1145/2522848.2522867},
	doi = {10.1145/2522848.2522867},
	abstract = {Evaluating human machine interaction in the case of multimodal systems is often a difficult task involving the monitoring of multiple sources, data fusion and results interpretation. While subtasks are highly dependent on the specific goal of the application and on the available interaction modalities, it is possible to formalize this workflow into a standard process and to consider a generic measure to estimate the ease of use of a specific application. In this work, we present {CoWME}, a modular software architecture describing multimodal human machine interaction evaluation, from data collection to final evaluation, in a formal way, in terms of cognitive workload. Communication protocols between modules are described in {XML} while data fusion is delegated to a configurable rule engine. An interface module is introduced between the monitoring modules and the rule engine to collect and summarize data streams for cognitive workload evaluation. We present a deployment example showing how this architecture is deployed by monitoring an interactive session with an Android application taking into account stressed speech detection, mydriasis and touch analysis.},
	urldate = {2013-12-25},
	booktitle = {Proceedings of the 15th {ACM} on International Conference on Multimodal Interaction},
	publisher = {{ACM}},
	author = {Calandra, Davide Maria and Caso, Antonio and Cutugno, Francesco and Origlia, Antonio and Rossi, Silvia},
	year = {2013},
	keywords = {cognitive workload, eyetracking, speech, usability testing workflow},
	pages = {111–118},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/XI8MDGRN/Calandra et al. - 2013 - CoWME A General Framework to Evaluate Cognitive W.pdf:application/pdf}
},

@inproceedings{vspakov_enhanced_2012,
	address = {New York, {NY}, {USA}},
	series = {{UbiComp} '12},
	title = {Enhanced Gaze Interaction Using Simple Head Gestures},
	isbn = {978-1-4503-1224-0},
	url = {http://doi.acm.org/10.1145/2370216.2370369},
	doi = {10.1145/2370216.2370369},
	abstract = {We propose a combination of gaze pointing and head gestures for enhanced hands-free interaction. Instead of the traditional dwell-time selection method, we experimented with five simple head gestures: nodding, turning left/right, and tilting left/right. The gestures were detected from the eye-tracking data by a range-based algorithm, which was found accurate enough in recognizing nodding and left-directed gestures. The gaze estimation accuracy did not noticeably suffer from the quick head motions. Participants pointed to nodding as the best gesture for occasional selections tasks and rated the other gestures as promising methods for navigation (turning) and functional mode switching (tilting). In general, dwell time works well for repeated tasks such as eye typing. However, considering multimodal games or transient interactions in pervasive and mobile environments, we believe a combination of gaze and head interaction could potentially provide a natural and more accurate interaction method.},
	urldate = {2014-01-06},
	booktitle = {Proceedings of the 2012 {ACM} Conference on Ubiquitous Computing},
	publisher = {{ACM}},
	author = {{\textbackslash}{vSpakov}, Oleg and Majaranta, Päivi},
	year = {2012},
	keywords = {dwell time, eye tracking, head gestures, selection},
	pages = {705–710},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/R73USQIH/vSpakov and Majaranta - 2012 - Enhanced Gaze Interaction Using Simple Head Gestur.pdf:application/pdf}
},

@article{derry_evaluating_2009,
	title = {Evaluating Head Gestures for Panning 2-D Spatial Information},
	url = {http://digitalcommons.calpoly.edu/cgi/viewcontent.cgi?article=1207&context=theses},
	urldate = {2013-11-13},
	journal = {Master's Theses and Project Reports},
	author = {Derry, Matthew O.},
	year = {2009},
	pages = {193}
},

@inproceedings{mardanbegi_eye-based_2012,
	address = {New York, {NY}, {USA}},
	series = {{ETRA} '12},
	title = {Eye-based Head Gestures},
	isbn = {978-1-4503-1221-9},
	url = {http://doi.acm.org/10.1145/2168556.2168578},
	doi = {10.1145/2168556.2168578},
	abstract = {A novel method for video-based head gesture recognition using eye information by an eye tracker has been proposed. The method uses a combination of gaze and eye movement to infer head gestures. Compared to other gesture-based methods a major advantage of the method is that the user keeps the gaze on the interaction object while interacting. This method has been implemented on a head-mounted eye tracker for detecting a set of predefined head gestures. The accuracy of the gesture classifier is evaluated and verified for gaze-based interaction in applications intended for both large public displays and small mobile phone screens. The user study shows that the method detects a set of defined gestures reliably.},
	urldate = {2014-01-06},
	booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications},
	publisher = {{ACM}},
	author = {Mardanbegi, Diako and Hansen, Dan Witzner and Pederson, Thomas},
	year = {2012},
	keywords = {eye tracker, gaze interaction, head gestures, interaction},
	pages = {139–146},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/SHIZ969U/Mardanbegi et al. - 2012 - Eye-based Head Gestures.pdf:application/pdf}
},

@article{kajastila_eyes-free_2013,
	title = {Eyes-free Interaction with Free-hand Gestures and Auditory Menus},
	volume = {71},
	issn = {1071-5819},
	url = {http://dx.doi.org/10.1016/j.ijhcs.2012.11.003},
	doi = {10.1016/j.ijhcs.2012.11.003},
	abstract = {Auditory interfaces can overcome visual interfaces when a primary task, such as driving, competes for the attention of a user controlling a device, such as radio. In emerging interfaces enabled by camera tracking, auditory displays may also provide viable alternatives to visual displays. This paper presents a user study of interoperable auditory and visual menus, in which control gestures remain the same in the visual and the auditory domain. Tested control methods included a novel free-hand gesture interaction with camera-based tracking, and touch screen interaction with a tablet. The task of the participants was to select numbers from a visual or an auditory menu including a circular layout and a numeric keypad layout. Results show, that even with participant's full attention to the task, the performance and accuracy of the auditory interface are the same or even slightly better than the visual when controlled with free-hand gestures. The auditory menu was measured to be slower in touch screen interaction, but questionnaire revealed that over half of the participants felt that the circular auditory menu was faster than the visual menu. Furthermore, visual and auditory feedback in touch screen interaction with numeric layout was measured fastest, touch screen with circular menu second fastest, and the free-hand gesture interface was slowest. The results suggest that auditory menus can potentially provide a fast and desirable interface to control devices with free-hand gestures.},
	number = {5},
	urldate = {2014-01-08},
	journal = {Int. J. Hum.-Comput. Stud.},
	author = {Kajastila, Raine and Lokki, Tapio},
	month = may,
	year = {2013},
	keywords = {Auditory display, Auditory menu, Eyes free, Free-hand gestures, interaction, Spatial sound},
	pages = {627–640}
},

@inproceedings{vazquez-alvarez_eyes-free_2011,
	title = {Eyes-free multitasking: the effect of cognitive load on mobile spatial audio interfaces},
	shorttitle = {Eyes-free multitasking},
	url = {http://dl.acm.org/citation.cfm?id=1979258},
	urldate = {2014-01-07},
	booktitle = {Proceedings of the {SIGCHI} Conference on Human Factors in Computing Systems},
	author = {Vazquez-Alvarez, Yolanda and Brewster, Stephen A.},
	year = {2011},
	pages = {2173–2176},
	file = {CHI2011_yolanda.pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/BRH6U4F9/CHI2011_yolanda.pdf:application/pdf}
},

@inproceedings{lu_head_2005,
	title = {Head nod and shake recognition based on multi-view model and hidden Markov model},
	doi = {10.1109/CGIV.2005.41},
	abstract = {Head gestures such as nodding and shaking are often used as one of human body languages for communication with each other, and their recognition plays an important role in the advancement of human-computer interaction. As head gesture is the continuous motion on the sequential time series, the key problems of recognition are to track multi-view head and understand the head pose switch. This paper presents a novel approach to recognize the nod and shake in the interactive computer environment. First, head poses are detected by multi-view model ({MVM)} and then hidden Markov model ({HMM)} are used as head gesture statistic inference model for gesture recognition. Experimental results show that the approach is effective and real time.},
	booktitle = {International Conference on Computer Graphics, Imaging and Vision: New Trends, 2005},
	author = {Lu, Peng and Zhang, Mandun and Zhu, Xinshan and Wang, Yangsheng},
	year = {2005},
	keywords = {Biological system modeling, Detectors, gesture recognition, Hardware, Head, head gesture statistic inference model, head nod recognition, head pose switch, head shake recognition, hidden Markov model, hidden Markov models, {HMM}, human body language, human computer interaction, human-computer interaction, interactive computer environment, Maximum likelihood detection, multiview model, sequential time series, Statistics, Switches, time series, Tracking},
	pages = {61--64},
	file = {IEEE Xplore Abstract Record:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/4S8K5HSD/articleDetails.html:text/html}
},

@inproceedings{taib_integrating_2008,
	address = {Berlin, Heidelberg},
	series = {{MLMI'07}},
	title = {Integrating Semantics into Multimodal Interaction Patterns},
	isbn = {3-540-78154-4, 978-3-540-78154-7},
	url = {http://dl.acm.org/citation.cfm?id=1787422.1787434},
	abstract = {A user experiment on multimodal interaction (speech, hand position and hand shapes) to study two major relationships: between the level of cognitive load experienced by users and the resulting multimodal interaction patterns; and how the semantics of the information being conveyed affected those patterns. We found that as cognitive load increases, users' multimodal productions tend to become semantically more complementary and less redundant across modalities. This validates cognitive load theory as a theoretical background for understanding the occurrence of particular kinds of multimodal productions. Moreover, results indicate a significant relationship between the temporal multimodal integration pattern (7 patterns in this experiment) and the semantics of the command being issued by the user (4 types of commands), shedding new light on previous research findings that assign a unique temporal integration pattern to any given subject regardless of the communication taking place.},
	urldate = {2014-01-06},
	booktitle = {Proceedings of the 4th International Conference on Machine Learning for Multimodal Interaction},
	publisher = {Springer-Verlag},
	author = {Taib, Ronnie and Ruiz, Natalie},
	year = {2008},
	pages = {96–107}
},

@article{kajastila_interaction_2013,
	title = {Interaction with eyes-free and gestural interfaces},
	url = {https://aaltodoc.aalto.fi/handle/123456789/7720},
	urldate = {2014-01-07},
	author = {Kajastila, Raine},
	year = {2013},
	file = {isbn9789526050034.pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/483GFBGQ/isbn9789526050034.pdf:application/pdf}
},

@inproceedings{brewster_multimodaleyes-freeinteraction_2003,
	title = {Multimodal'eyes-free'interaction techniques for wearable devices},
	url = {http://dl.acm.org/citation.cfm?id=642694},
	urldate = {2014-01-07},
	booktitle = {Proceedings of the {SIGCHI} conference on Human factors in computing systems},
	author = {Brewster, Stephen and Lumsden, Joanna and Bell, Marek and Hall, Malcolm and Tasker, Stuart},
	year = {2003},
	pages = {473–480},
	file = {CHI2003.pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/D7JHEUEX/CHI2003.pdf:application/pdf}
},

@article{sawhney_nomadic_2000,
	title = {Nomadic radio: speech and audio interaction for contextual messaging in nomadic environments},
	volume = {7},
	shorttitle = {Nomadic radio},
	url = {http://dl.acm.org/citation.cfm?id=355327},
	number = {3},
	urldate = {2014-01-07},
	journal = {{ACM} transactions on Computer-Human interaction ({TOCHI)}},
	author = {Sawhney, Nitin and Schmandt, Chris},
	year = {2000},
	pages = {353–383},
	file = {sawhney_ToCHI00_nomadic_radio.pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/QXFZ67DZ/sawhney_ToCHI00_nomadic_radio.pdf:application/pdf}
},

@article{brewster_overcoming_2002,
	title = {Overcoming the lack of screen space on mobile computers},
	volume = {6},
	url = {http://dl.acm.org/citation.cfm?id=594356},
	number = {3},
	urldate = {2014-01-07},
	journal = {Personal and Ubiquitous Computing},
	author = {Brewster, Stephen},
	year = {2002},
	pages = {188–205},
	file = {overcoming1.pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/RCFTSVKT/overcoming1.pdf:application/pdf}
},

@inproceedings{zhang_real-time_2010,
	address = {New York, {NY}, {USA}},
	series = {{ICMI-MLMI} '10},
	title = {Real-time Adaptive Behaviors in Multimodal Human-avatar Interactions},
	isbn = {978-1-4503-0414-6},
	url = {http://doi.acm.org/10.1145/1891903.1891909},
	doi = {10.1145/1891903.1891909},
	abstract = {Multimodal interaction in everyday life seems so effortless. However, a closer look reveals that such interaction is indeed complex and comprises multiple levels of coordination, from high-level linguistic exchanges to low-level couplings of momentary bodily movements both within an agent and across multiple interacting agents. A better understanding of how these multimodal behaviors are coordinated can provide insightful principles to guide the development of intelligent multimodal interfaces. In light of this, we propose and implement a research framework in which human participants interact with a virtual agent in a virtual environment. Our platform allows the virtual agent to keep track of the user's gaze and hand movements in real time, and adjust his own behaviors accordingly. An experiment is designed and conducted to investigate adaptive user behaviors in a human-agent joint attention task. Multimodal data streams are collected in the study including speech, eye gaze, hand and head movements from both the human user and the virtual agent, which are then analyzed to discover various behavioral patterns. Those patterns show that human participants are highly sensitive to momentary multimodal behaviors generated by the virtual agent and they rapidly adapt their behaviors accordingly. Our results suggest the importance of studying and understanding real-time adaptive behaviors in human-computer multimodal interactions.},
	urldate = {2014-01-06},
	booktitle = {International Conference on Multimodal Interfaces and the Workshop on Machine Learning for Multimodal Interaction},
	publisher = {{ACM}},
	author = {Zhang, Hui and Fricker, Damian and Smith, Thomas G. and Yu, Chen},
	year = {2010},
	keywords = {embodied agent, multimodal interaction, virtual human, visualization},
	pages = {4:1–4:8},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/UHVMCTEU/Zhang et al. - 2010 - Real-time Adaptive Behaviors in Multimodal Human-a.pdf:application/pdf}
},

@inproceedings{dong_recognition_2007,
	title = {Recognition of Multi-Pose Head Gestures in Human Conversations},
	doi = {10.1109/ICIG.2007.176},
	abstract = {We address the problem of recognizing multi-pose head nodding and shaking gestures in human conversations. Existing methods mainly recognize head gestures in restricted environments like human robot interaction, where face poses are near frontal and head motions are not natural. However, in human conversations, faces of subjects might be in arbitrary poses while head gestures are often subtle. Since the face pose change and head gesture movement are of different scale, we propose to track the face of varied poses with a mixed-state particle filter and detect the subtle head movement by a Kanade-Lucas-Tomasi tracker. The motion patterns in both horizontal and vertical directions are detected and then head gestures are analyzed by a Finite State Machine. Experiments on natural human conversations demonstrated the effectiveness of our method.},
	booktitle = {Fourth International Conference on Image and Graphics, 2007. {ICIG} 2007},
	author = {Dong, Ligeng and Jin, Yuxin and Tao, Linmi and Xu, Guangyou},
	year = {2007},
	keywords = {Automata, Face detection, face recognition, finite state machine, finite state machines, gesture recognition, head gesture movement, human computer interaction, human conversations, human robot interaction, Kanade-Lucas-Tomasi tracker, Magnetic heads, mixed-state particle filter, Motion analysis, Motion detection, multipose head gestures recognition, particle filtering (numerical methods), Particle filters, Particle tracking, Pattern analysis, robot vision, shaking gestures},
	pages = {650--654},
	file = {IEEE Xplore Abstract Record:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/2XRU37B8/articleDetails.html:text/html}
},

@inproceedings{geelhoed_safety_2000,
	address = {London, {UK}, {UK}},
	series = {{HUC} '00},
	title = {Safety and Comfort of Eyeglass Displays},
	isbn = {3-540-41093-7},
	url = {http://dl.acm.org/citation.cfm?id=647986.743887},
	abstract = {An eyeglass display features two micro displays and both eyes are presented with the same image. This configuration is safer than virtual reality helmets, which give rise to severe vision problems and nausea. They are also safer than monocular displays, which impair judgement of distance, speed and size. Current eyeglass display products are occluded and are likely to produce vergence lock, a potential health hazard. We suggest that eyeglass displays should allow good peripheral vision and should be used in relatively light environments to counteract vergence lock.},
	urldate = {2014-01-07},
	booktitle = {Proceedings of the {2Nd} International Symposium on Handheld and Ubiquitous Computing},
	publisher = {Springer-Verlag},
	author = {Geelhoed, Erik and Falahee, Marie and Latham, Kezzy},
	year = {2000},
	pages = {236–247}
},

@article{gaver_sonicfinder:_1989,
	title = {The {SonicFinder:} An Interface That Uses Auditory Icons},
	volume = {4},
	issn = {0737-0024},
	shorttitle = {The {SonicFinder}},
	url = {http://dx.doi.org/10.1207/s15327051hci0401_3},
	doi = {10.1207/s15327051hci0401_3},
	abstract = {The appropriate use of nonspeech sounds has the potential to add a great deal to the functionality of computer interfaces. Sound is a largely unexploited medium of output, even though it plays an integral role in our everyday encounters with the world, a role that is complementary to vision. Sound should be used in computers as it is in the world, where it conveys information about the nature of sound-producing events. Such a strategy leads to auditory icons, which are everyday sounds meant to convey information about computer events by analogy with everyday events. Auditory icons are an intuitively accessible way to use sound to provide multidimensional, organized information to users. These ideas are instantiated in the {SonicFinder}, which is an auditory interface I developed at Apple Computer, Inc. In this interface, information is conveyed using auditory icons as well as standard graphical feedback. I discuss how events are mapped to auditory icons in the {SonicFinder}, and illustrate how sound is used by describing a typical interaction with this interface. Two major gains are associated with using sound in this interface: an increase in direct engagement with the model world of the computer and an added flexibility for users in getting information about that world. These advantages seem to be due to the iconic nature of the mappings used between sound and the information it is to convey. I discuss sound effects and source metaphors as methods of extending auditory icons beyond the limitations implied by literal mappings, and I speculate on future directions for such interfaces.},
	number = {1},
	urldate = {2014-01-08},
	journal = {Hum.-Comput. Interact.},
	author = {Gaver, William W.},
	month = mar,
	year = {1989},
	pages = {67–94}
},

@article{graham_use_1999,
	title = {Use of auditory icons as emergency warnings: evaluation within a vehicle collision avoidance application},
	volume = {42},
	issn = {0014-0139},
	shorttitle = {Use of auditory icons as emergency warnings},
	doi = {10.1080/001401399185108},
	abstract = {In the context of emergency warnings, auditory icons, which convey information about system events by analogy with everyday events, have the potential to be understood more quickly and easily than abstract sounds. To test this proposal, an experiment was carried out to evaluate the use of auditory icons for an in-vehicle collision avoidance application. Two icons, the sounds of a car horn and of skidding tyres, were compared with two conventional warnings, a simple tone and a voice saying 'ahead'. Participants sat in an experimental vehicle with a road scene projected ahead, and they were required to brake in response to on-screen collision situations and their accompanying warning sounds. The auditory icons produced significantly faster reaction times than the conventional warnings, but suffered from more inappropriate responses, where drivers reacted with a brake press to a non-collision situation. The findings are explained relative to the perceived urgency and inherent meaning of each sound. It is argued that optimal warnings could be achieved by adjusting certain sound attributes of auditory icons, as part of a structured, user-centred design and evaluation procedure.},
	language = {eng},
	number = {9},
	journal = {Ergonomics},
	author = {Graham, R},
	month = sep,
	year = {1999},
	note = {{PMID:} 10503056},
	keywords = {Accidents, Traffic, Acoustic Stimulation, Adult, Analysis of Variance, Equipment Design, Female, Humans, Male, Protective Devices, Reaction Time},
	pages = {1233--1248}
},

@inproceedings{sandberg_using_2006,
	title = {Using {3D} audio guidance to locate indoor static objects},
	volume = {50},
	url = {http://pro.sagepub.com/content/50/16/1581.short},
	urldate = {2013-11-12},
	booktitle = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	author = {Sandberg, Samuel and H{\textbackslash}aakansson, Calle and Elmqvist, Niklas and Tsigas, Philippas and Chen, Fang},
	year = {2006},
	keywords = {audio navigation},
	pages = {1581–1584},
	file = {3daudioloc.pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/WFT6SGT6/3daudioloc.pdf:application/pdf}
},

@article{pascoe_using_2000,
	title = {Using while moving: {HCI} issues in fieldwork environments},
	volume = {7},
	shorttitle = {Using while moving},
	url = {http://dl.acm.org/citation.cfm?id=355329},
	number = {3},
	urldate = {2014-01-08},
	journal = {{ACM} Transactions on Computer-Human Interaction ({TOCHI)}},
	author = {Pascoe, Jason and Ryan, Nick and Morse, David},
	year = {2000},
	pages = {417–437},
	file = {UsingWhileMoving.pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/7S2ZHUXR/UsingWhileMoving.pdf:application/pdf}
},

@inproceedings{ur_reahman_vibrotactile_2009,
	title = {Vibrotactile rendering of head gestures for controlling electric wheelchair},
	doi = {10.1109/ICSMC.2009.5346213},
	abstract = {We have developed a head gesture controlled electric wheelchair system to aid persons with severe disabilities. Realtime range information obtained from a stereo camera is used to locate and segment the face images of the user from the sensed video. We use an Isomap based nonlinear manifold learning map of facial textures for head pose estimation. Our system is a non-contact vision system, making it much more convenient to use. The user is only required to gesture his/her head to command the wheelchair. To overcome problems with a non responding system, it is necessary to notify the user of the exact system state while the system is in use. In this paper, we explore the use of vibrotactile rendering of head gestures as feedback. Three different feedback systems are developed and tested, audio stimuli, vibrotactile stimuli and audio plus vibrotactile stimuli. We have performed user tests to study the usability of these three display methods. The usability studies show that the method using both audio plus vibrotactile response outperforms the other methods (i.e. audio stimuli, vibrotactile stimuli response).},
	booktitle = {{IEEE} International Conference on Systems, Man and Cybernetics, 2009. {SMC} 2009},
	author = {ur Reahman, S. and Raytchev, B. and Yoda, I. and Liu, Li},
	year = {2009},
	keywords = {Cameras, Control systems, disabled person aid, electric wheelchair control, extended Isomap, Face detection, face image segmentation, face recognition, facial texture, feedback, feedback system, gesture recognition, handicapped aids, haptic interfaces, Head, head gesture, head gesture recognition, image segmentation, isomap based nonlinear manifold learning map, Machine vision, medical control systems, Multidimensional Scaling ({MDS)}, noncontact vision system, pose estimation, realtime range information, Rendering (computer graphics), stereo camera, stereo image processing, Usability, vibrotactile rendering, video sensing, video signal processing, wheelchair system, Wheelchairs},
	pages = {413--417},
	file = {IEEE Xplore Abstract Record:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/BAVGQ53J/abs_all.html:text/html;IEEE Xplore Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/I98BSP5J/ur Reahman et al. - 2009 - Vibrotactile rendering of head gestures for contro.pdf:application/pdf}
}