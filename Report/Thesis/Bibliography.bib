
@book{begault_3dd_1994,
	address = {San Diego, {CA}, {USA}},
	title = {{3DD} Sound for Virtual Reality and Multimedia},
	isbn = {0-12-084735-3},
	publisher = {Academic Press Professional, Inc.},
	author = {Begault, Durand R.},
	year = {1994},
	file = {download.pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/5Q7HFNH2/download.pdf:application/pdf}
},

@inproceedings{pasquero_haptic_2011,
	address = {New York, {NY}, {USA}},
	series = {{CHI} '11},
	title = {A Haptic Wristwatch for Eyes-free Interactions},
	isbn = {978-1-4503-0228-9},
	url = {http://doi.acm.org/10.1145/1978942.1979425},
	doi = {10.1145/1978942.1979425},
	abstract = {We present a haptic wristwatch prototype that makes it possible to acquire information from a companion mobile device through simple eyes-free gestures. The wristwatch we have built uses a custom-made piezoelectric actuator combined with sensors to create a natural, inconspicuous, gesture-based interface. Feedback is returned to the user in the form of haptic stimuli that are delivered to the wrist. We evaluated the capabilities and limitations of our prototype through two user experiments. One experiment verified that the apparatus could be used as a tactile notification mechanism. The other experiment assessed the feasibility of using a cover-and-hold gesture on the wristwatch to obtain numerical data tactually. Results from the numerosity experiment and feedback from participants prompted us to redesign the cover-and-hold gesture to provide users with additional control over the interaction. We qualitatively evaluated the redesigned interaction by handing the prototype to users so that they could use it in a realistic work environment. Taken together, results from the experiments and the validation process indicate that a wrist accessory can be effectively used to perform discreet, closed-loop, eyes-free interactions with a mobile device.},
	urldate = {2014-02-03},
	booktitle = {Proceedings of the {SIGCHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Pasquero, Jerome and Stobbe, Scott J. and Stonehouse, Noel},
	year = {2011},
	keywords = {Eyes-free interaction, haptic interface, non-visual gestures, wearable computing},
	pages = {3257–3266},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/XIBB7K7P/Pasquero et al. - 2011 - A Haptic Wristwatch for Eyes-free Interactions.pdf:application/pdf}
},

@inproceedings{nazari_shirehjini_mobile_2009,
	title = {A mobile {3D} user interface for interaction with ambient audio visual environments},
	doi = {10.1109/HAVE.2009.5356120},
	abstract = {This paper describes concepts, design, implementation, and performance evaluation of a {3D-based} user interface for accessing Ambient Intelligence Environments ({AmIE).} The generic interaction model of the described work addresses some major challenges of Human-Environment-Interaction such as cognitive overload and manual device selection, loss of user control, missing system image or over-automation. {3D} visualization and {3D} {UI}, acting as the central feature of the system, create a logical link between physical devices and their virtual representation on the user's {PDA} or other mobile devices. By doing so, the user can easily identify a device within the environment based on its position, orientation, and form, and access the identified devices through the {3D} interface for direct manipulation within the scene.},
	booktitle = {{IEEE} International Workshop on Haptic Audio visual Environments and Games, 2009. {HAVE} 2009},
	author = {Nazari Shirehjini, {A.A.} and Shirmohammadi, S.},
	month = nov,
	year = {2009},
	keywords = {{3D} user interface techniques, {3D} visualization, ambient audio visual environments, Ambient intelligence, ambient intelligence environments, applications of {3D} {UI} techniques, audio-visual systems, Cameras, central feature system, cognitive overload, cognitive systems, Collaborative work, Control system synthesis, data visualisation, Design engineering, generic interaction model, human computer interaction, human environment interaction, Human-Computer-Interaction, Information technology, Layout, manual device selection, missing system image, Mobile {3D} {UIs}, mobile {3D} user interface, mobile computing, mobile devices, notebook computers, {PDA} users, speech, user control loss, User interfaces, Virtual environment},
	pages = {186--191},
	file = {IEEE Xplore Abstract Record:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/EPW33MUP/articleDetails.html:text/html;IEEE Xplore Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/SVXZK46A/Nazari Shirehjini and Shirmohammadi - 2009 - A mobile 3D user interface for interaction with am.pdf:application/pdf}
},

@inproceedings{lumsden_paradigm_2003,
	address = {Toronto, Ontario, Canada},
	series = {{CASCON} '03},
	title = {A Paradigm Shift: Alternative Interaction Techniques for Use with Mobile \& Wearable Devices},
	shorttitle = {A Paradigm Shift},
	url = {http://dl.acm.org/citation.cfm?id=961322.961355},
	abstract = {Desktop user interface design originates from the fact that users are stationary and can devote all of their visual resource to the application with which they are interacting. In contrast, users of mobile and wearable devices are typically in motion whilst using their device which means that they cannot devote all or any of their visual resource to interaction with the mobile application -- it must remain with the primary task, often for safety reasons. Additionally, such devices have limited screen real estate and traditional input and output capabilities are generally restricted. Consequently, if we are to develop effective applications for use on mobile or wearable technology, we must embrace a paradigm shift with respect to the interaction techniques we employ for communication with such {devices.This} paper discusses why it is necessary to embrace a paradigm shift in terms of interaction techniques for mobile technology and presents two novel multimodal interaction techniques which are effective alternatives to traditional, visual-centric interface designs on mobile devices as empirical examples of the potential to achieve this shift.},
	urldate = {2014-01-29},
	booktitle = {Proceedings of the 2003 Conference of the Centre for Advanced Studies on Collaborative Research},
	publisher = {{IBM} Press},
	author = {Lumsden, Joanna and Brewster, Stephen},
	year = {2003},
	pages = {197–210},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/86XMQFIM/Lumsden and Brewster - 2003 - A Paradigm Shift Alternative Interaction Techniqu.pdf:application/pdf}
},

@incollection{pielot_tactile_2011,
	series = {Lecture Notes in Computer Science},
	title = {A Tactile Compass for Eyes-Free Pedestrian Navigation},
	copyright = {©2011 {IFIP} International Federation for Information Processing},
	isbn = {978-3-642-23770-6, 978-3-642-23771-3},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-23771-3_47},
	abstract = {This paper reports from the first systematic investigation on how to guide people to a destination using the haptic feedback of a mobile phone and its experimental evaluation. The aim was to find a navigation aid that works hands-free, reduces the users’ distraction, and can be realised with widely available handheld devices. To explore the design space we developed and tested different prototypes. Drawing on the results of these tests we present the concept of a tactile compass, which encodes the direction of a location "as the crow flies" in rhythmic patterns and its distance in the pause between two patterns. This paper also reports from the first experimental comparison of such tactile displays with visual navigation systems. The tactile compass was used to continuously display the location of a destination from the user’s perspective (e.g. ahead, close). In a field experiment including the tactile compass and an interactive map three conditions were investigated: tactile only, visual only, and combined. The results provide evidence that cueing spatial locations in vibration patterns can form an effective and efficient navigation aid. Between the conditions, no significant differences in the navigation performance were found. The tactile compass used alone could significantly reduce the amount of distractive interaction and together with the map it improved the participants’ confidence in the navigation system.},
	number = {6947},
	urldate = {2014-02-03},
	booktitle = {Human-Computer Interaction – {INTERACT} 2011},
	publisher = {Springer Berlin Heidelberg},
	author = {Pielot, Martin and Poppinga, Benjamin and Heuten, Wilko and Boll, Susanne},
	editor = {Campos, Pedro and Graham, Nicholas and Jorge, Joaquim and Nunes, Nuno and Palanque, Philippe and Winckler, Marco},
	month = jan,
	year = {2011},
	keywords = {Artificial Intelligence (incl. Robotics), Computers and Education, Computers and Society, Information Systems Applications ({incl.Internet)}, Multi-Modal Interface, Novel User Interfaces and Interaction Techniques, Software Engineering, User Interfaces and Human Computer Interaction, We Mobile Accessibility},
	pages = {640--656},
	file = {Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/XE9DGQI9/Pielot et al. - 2011 - A Tactile Compass for Eyes-Free Pedestrian Navigat.pdf:application/pdf;Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/5V6DKPGR/10.html:text/html}
},

@article{sodnik_user_2008,
	title = {A user study of auditory versus visual interfaces for use while driving},
	volume = {66},
	issn = {10715819},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1071581907001553},
	doi = {10.1016/j.ijhcs.2007.11.001},
	number = {5},
	urldate = {2014-02-03},
	journal = {International Journal of Human-Computer Studies},
	author = {Sodnik, Jaka and Dicke, Christina and Tomažič, Sašo and Billinghurst, Mark},
	month = may,
	year = {2008},
	pages = {318--332},
	file = {1-s2.0-S1071581907001553-main.pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/W5KP2I2U/1-s2.0-S1071581907001553-main.pdf:application/pdf}
},

@inproceedings{akl_accelerometer-based_2010,
	title = {Accelerometer-based gesture recognition via dynamic-time warping, affinity propagation, \#x00026; compressive sensing},
	doi = {10.1109/ICASSP.2010.5495895},
	abstract = {We propose a gesture recognition system based primarily on a single 3-axis accelerometer. The system employs dynamic time warping and affinity propagation algorithms for training and utilizes the sparse nature of the gesture sequence by implementing compressive sensing for gesture recognition. A dictionary of 18 gestures is defined and a database of over 3,700 repetitions is created from 7 users. Our dictionary of gestures is the largest in published studies related to acceleration-based gesture recognition, to the best of our knowledge. The proposed system achieves almost perfect user-dependent recognition and a user-independent recognition accuracy that is competitive with the statistical methods that require significantly a large number of training samples and with the other accelerometer-based gesture recognition systems available in literature.},
	booktitle = {2010 {IEEE} International Conference on Acoustics Speech and Signal Processing ({ICASSP)}},
	author = {Akl, A. and Valaee, S.},
	month = mar,
	year = {2010},
	keywords = {Acceleration, accelerometer-based gesture recognition, accelerometers, affinity propagation, compressive sensing, Computer interfaces, Databases, Dictionaries, Dynamic time warping, dynamic-time warping, gesture recognition, hidden Markov models, Humans, Pattern recognition, single 3-axis accelerometer, statistical analysis, statistical methods, Testing, user-dependent recognition},
	pages = {2270--2273},
	file = {IEEE Xplore Abstract Record:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/UWE5X76D/login.html:text/html;IEEE Xplore Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/TK8KKMHZ/Akl and Valaee - 2010 - Accelerometer-based gesture recognition via dynami.pdf:application/pdf}
},

@misc{apple_apple_2014,
	title = {Apple products},
	url = {https://www.apple.com/},
	abstract = {Apple designs and creates {iPod} and {iTunes}, Mac laptop and desktop computers, the {OS} X operating system, and the revolutionary {iPhone} and {iPad.}},
	urldate = {2014-03-11},
	author = {{Apple}},
	year = {2014},
	file = {Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/57MG5SU2/www.apple.com.html:text/html}
},

@inproceedings{schmandt_audiostreamer:_1995,
	address = {New York, {NY}, {USA}},
	series = {{CHI} '95},
	title = {{AudioStreamer:} Exploiting Simultaneity for Listening},
	isbn = {0-89791-755-3},
	shorttitle = {{AudioStreamer}},
	url = {http://doi.acm.org/10.1145/223355.223533},
	doi = {10.1145/223355.223533},
	urldate = {2014-03-30},
	booktitle = {Conference Companion on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Schmandt, Chris and Mullins, Atty},
	year = {1995},
	pages = {218–219},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/KMZHQ94A/Schmandt and Mullins - 1995 - AudioStreamer Exploiting Simultaneity for Listeni.pdf:application/pdf}
},

@article{gaver_auditory_1986,
	title = {Auditory Icons: Using Sound in Computer Interfaces},
	volume = {2},
	issn = {0737-0024},
	shorttitle = {Auditory Icons},
	url = {http://dx.doi.org/10.1207/s15327051hci0202_3},
	doi = {10.1207/s15327051hci0202_3},
	abstract = {There is growing interest in the use of sound to convey information in computer interfaces. The strategies employed thus far have been based on an understanding of sound that leads to either an arbitrary or metaphorical relation between the sounds used and the data to be represented. In this article, an alternative approach to the use of sound in computer interfaces is outlined, one that emphasizes the role of sound in conveying information about the world to the listener. According to this approach, auditory icons, caricatures of naturally occurring sounds, could be used to provide information about sources of data. Auditory icons provide a natural way to represent dimensional data as well as conceptual objects in a computer system. They allow categorization of data into distinct families, using a single sound. Perhaps the most important advantage of this strategy is that it is based on the way people listen to the world in their everyday lives.},
	number = {2},
	urldate = {2014-01-08},
	journal = {Hum.-Comput. Interact.},
	author = {Gaver, William W.},
	month = jun,
	year = {1986},
	pages = {167–177}
},

@misc{cyklistforbundet_bodetakster_2014,
	title = {Bødetakster for cyklister},
	url = {http://www.cyklistforbundet.dk/Alt-om-cykling/Love-og-regler/boedetakster},
	abstract = {Rigsadvokatens vejledende bødetakster for trafikforseelser for cyklister.},
	urldate = {2014-01-22},
	author = {{Cyklistforbundet}},
	year = {2014},
	file = {Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/MHEHMXQ8/boedetakster.html:text/html}
},

@inproceedings{strachan_bodyspace_2007,
	address = {New York, {NY}, {USA}},
	series = {{CHI} {EA} '07},
	title = {{BodySpace} Inferring Body Pose for Natural Control of a Music Player},
	isbn = {978-1-59593-642-4},
	shorttitle = {{BodySpace}},
	url = {http://doi.acm.org/10.1145/1240866.1240939},
	doi = {10.1145/1240866.1240939},
	abstract = {We describe the {BodySpace} system, which uses inertial sensing and pattern recognition to allow the gestural control of a music player by placing the device at different parts of the body. We demonstrate a new approach to the segmentation and recognition of gestures for this kind of application and show how simulated physical model-based techniques can shape gestural interaction.},
	urldate = {2014-03-07},
	booktitle = {{CHI} '07 Extended Abstracts on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Strachan, Steven and Murray-Smith, Roderick and {O'Modhrain}, Sile},
	year = {2007},
	keywords = {accelerometer, gesture recognition, interaction design, music player, Pattern recognition},
	pages = {2001–2006},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/62MZ9BPJ/Strachan et al. - 2007 - BodySpace Inferring Body Pose for Natural Control.pdf:application/pdf}
},

@book{morris_bodytalk:_1994,
	title = {Bodytalk: the meaning of human gestures},
	shorttitle = {Bodytalk},
	publisher = {Crown Publishers},
	author = {Morris, Desmond},
	year = {1994},
	file = {1995 - Bodytalk—The Meaning of Human Gestures - Desmond Morris.pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/UHWPE85K/1995 - Bodytalk—The Meaning of Human Gestures - Desmond Morris.pdf:application/pdf}
},

@article{nunes_cognitive_2002,
	title = {Cognitive demands of hands-free-phone conversation while driving},
	volume = {5},
	issn = {1369-8478},
	url = {http://www.sciencedirect.com/science/article/pii/S1369847802000128},
	doi = {10.1016/S1369-8478(02)00012-8},
	abstract = {In four field experiments the participants drove an instrumented car provided with a hands-free phone and performed several cognitive tasks while driving including phone conversations. The study focussed the cognitive component of the conversations, excluding dialling. The cognitive demands of the conversations were varied and in two of the experiments the same tasks had two versions: by phone and in live conversation with the experimenter in the car. Several dependent measures like visual search behaviour, driving speed, visual detection and response selection capacities and others were analysed. Like in previous experiments of the same authors the more demanding cognitive tasks produced higher interference effects, but when the same tasks performed by phone were compared with its live versions no differences were observed. Once the manual phone operation has been technically suppressed the risk of phone conversations relies on the demands of the message content and its equivalent to talking to a passenger. Implications for safety are discussed.},
	number = {2},
	urldate = {2014-03-17},
	journal = {Transportation Research Part F: Traffic Psychology and Behaviour},
	author = {Nunes, Luis and Recarte, Miguel Angel},
	month = jun,
	year = {2002},
	keywords = {Attention, Eye movements, Mental workload, Phone, Traffic},
	pages = {133--144},
	file = {ScienceDirect Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/RWE2XDT3/Nunes and Recarte - 2002 - Cognitive demands of hands-free-phone conversation.pdf:application/pdf;ScienceDirect Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/WI7I2A9K/S1369847802000128.html:text/html}
},

@inproceedings{kratz_combining_2013,
	address = {New York, {NY}, {USA}},
	series = {{IUI} '13},
	title = {Combining Acceleration and Gyroscope Data for Motion Gesture Recognition Using Classifiers with Dimensionality Constraints},
	isbn = {978-1-4503-1965-2},
	url = {http://doi.acm.org/10.1145/2449396.2449419},
	doi = {10.1145/2449396.2449419},
	abstract = {Motivated by the addition of gyroscopes to a large number of new smart phones, we study the effects of combining accelerometer and gyroscope data on the recognition rate of motion gesture recognizers with dimensionality constraints. Using a large data set of motion gestures we analyze results for the following algorithms: {Protractor3D}, Dynamic Time Warping ({DTW)} and Regularized Logistic Regression ({LR).} We chose to study these algorithms because they are relatively easy to implement, thus well suited for rapid prototyping or early deployment during prototyping stages. For use in our analysis, we contribute a method to extend {Protractor3D} to work with the {6D} data obtained by combining accelerometer and gyroscope data. Our results show that combining accelerometer and gyroscope data is beneficial also for algorithms with dimensionality constraints and improves the gesture recognition rate on our data set by up to 4\%.},
	urldate = {2014-02-06},
	booktitle = {Proceedings of the 2013 International Conference on Intelligent User Interfaces},
	publisher = {{ACM}},
	author = {Kratz, Sven and Rohs, Michael and Essl, Georg},
	year = {2013},
	keywords = {accelerometer, gesture recognition, gyroscope, mobile, motion gestures, sensor fusion},
	pages = {173–178},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/HRRW2E2S/Kratz et al. - 2013 - Combining Acceleration and Gyroscope Data for Moti.pdf:application/pdf}
},

@inproceedings{calandra_cowme:_2013,
	address = {New York, {NY}, {USA}},
	series = {{ICMI} '13},
	title = {{CoWME:} A General Framework to Evaluate Cognitive Workload During Multimodal Interaction},
	isbn = {978-1-4503-2129-7},
	shorttitle = {{CoWME}},
	url = {http://doi.acm.org/10.1145/2522848.2522867},
	doi = {10.1145/2522848.2522867},
	abstract = {Evaluating human machine interaction in the case of multimodal systems is often a difficult task involving the monitoring of multiple sources, data fusion and results interpretation. While subtasks are highly dependent on the specific goal of the application and on the available interaction modalities, it is possible to formalize this workflow into a standard process and to consider a generic measure to estimate the ease of use of a specific application. In this work, we present {CoWME}, a modular software architecture describing multimodal human machine interaction evaluation, from data collection to final evaluation, in a formal way, in terms of cognitive workload. Communication protocols between modules are described in {XML} while data fusion is delegated to a configurable rule engine. An interface module is introduced between the monitoring modules and the rule engine to collect and summarize data streams for cognitive workload evaluation. We present a deployment example showing how this architecture is deployed by monitoring an interactive session with an Android application taking into account stressed speech detection, mydriasis and touch analysis.},
	urldate = {2013-12-25},
	booktitle = {Proceedings of the 15th {ACM} on International Conference on Multimodal Interaction},
	publisher = {{ACM}},
	author = {Calandra, Davide Maria and Caso, Antonio and Cutugno, Francesco and Origlia, Antonio and Rossi, Silvia},
	year = {2013},
	keywords = {cognitive workload, eyetracking, speech, usability testing workflow},
	pages = {111–118},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/XI8MDGRN/Calandra et al. - 2013 - CoWME A General Framework to Evaluate Cognitive W.pdf:application/pdf}
},

@inproceedings{witt_designing_2006,
	title = {Designing a wearable user interface for hands-free interaction in maintenance applications},
	doi = {10.1109/PERCOMW.2006.39},
	abstract = {One challenge in wearable computing is the design of proper user interfaces and interaction concepts for applications. This paper discusses the design of hands-free wearable user interfaces and shows an example interface for an aircraft maintenance application. The user interface we present uses a wireless data glove for interaction},
	booktitle = {Fourth Annual {IEEE} International Conference on Pervasive Computing and Communications Workshops, 2006. {PerCom} Workshops 2006},
	author = {Witt, H. and Nicolai, T. and Kenn, Holger},
	month = mar,
	year = {2006},
	keywords = {aerospace computing, Aerospace electronics, aircraft maintenance, Aircraft manufacture, Application software, Computer displays, Computer interfaces, data gloves, hands-free interaction, interactive systems, Mice, Personal digital assistants, User interfaces, wearable computers, wearable user interface, wireless data glove},
	pages = {4 pp.--655},
	file = {IEEE Xplore Abstract Record:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/IXUS8ZG3/abs_all.html:text/html;IEEE Xplore Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/H9ZAUIV2/Witt et al. - 2006 - Designing a wearable user interface for hands-free.pdf:application/pdf}
},

@incollection{oakley_designing_2007,
	series = {Lecture Notes in Computer Science},
	title = {Designing Eyes-Free Interaction},
	copyright = {©2007 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-76701-5, 978-3-540-76702-2},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-76702-2_13},
	abstract = {As the form factors of computational devices diversify, the concept of eyes-free interaction is becoming increasingly relevant: it is no longer hard to imagine use scenarios in which screens are inappropriate. However, there is currently little consensus about this term. It is regularly employed in different contexts and with different intents. One key consequence of this multiplicity of meanings is a lack of easily accessible insights into how to best build an eyes-free system. This paper seeks to address this issue by thoroughly reviewing the literature, proposing a concise definition and presenting a set of design principles. The application of these principles is then elaborated through a case study of the design of an eyes-free motion input system for a wearable device.},
	number = {4813},
	urldate = {2014-02-03},
	booktitle = {Haptic and Audio Interaction Design},
	publisher = {Springer Berlin Heidelberg},
	author = {Oakley, Ian and Park, Jun-Seok},
	editor = {Oakley, Ian and Brewster, Stephen},
	month = jan,
	year = {2007},
	keywords = {Computers and Education, Computers and Society, design principles, Eyes-free interaction, Information Storage and Retrieval, Information Systems Applications ({incl.Internet)}, motion input, User Interfaces and Human Computer Interaction},
	pages = {121--132},
	file = {Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/HIB2RI5R/Oakley and Park - 2007 - Designing Eyes-Free Interaction.pdf:application/pdf;Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/Q57JM4BE/10.html:text/html}
},

@book{benyon_designing_2010,
	address = {Harlow, England; New York},
	title = {Designing interactive systems: a comprehensive guide to {HCI} and interaction design},
	isbn = {9780321435330 0321435338},
	shorttitle = {Designing interactive systems},
	abstract = {The authors in this work focus on and explore human computer interaction ({HCI)} by bringing together the best practice and experience from {HCI} and interaction design.},
	language = {English},
	publisher = {Addison Wesley},
	author = {Benyon, David},
	year = {2010},
	file = {Designing Interactive Systems - David Benyon.pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/93TFG5Q2/Designing Interactive Systems - David Benyon.pdf:application/pdf}
},

@inproceedings{zhao_earpod:_2007,
	address = {New York, {NY}, {USA}},
	series = {{CHI} '07},
	title = {Earpod: Eyes-free Menu Selection Using Touch Input and Reactive Audio Feedback},
	isbn = {978-1-59593-593-9},
	shorttitle = {Earpod},
	url = {http://doi.acm.org/10.1145/1240624.1240836},
	doi = {10.1145/1240624.1240836},
	abstract = {We present the design and evaluation of {earPod:} an eyes-free menu technique using touch input and reactive auditory feedback. Studies comparing {earPod} with an {iPod-like} visual menu technique on reasonably-sized static menus indicate that they are comparable in accuracy. In terms of efficiency (speed), {earPod} is initially slower, but outperforms the visual technique within 30 minutes of practice. Our results indicate that {earPod} is potentially a reasonable eyes-free menu technique for general use, and is a particularly exciting technique for use in mobile device interfaces.},
	urldate = {2014-02-03},
	booktitle = {Proceedings of the {SIGCHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Zhao, Shengdong and Dragicevic, Pierre and Chignell, Mark and Balakrishnan, Ravin and Baudisch, Patrick},
	year = {2007},
	keywords = {Auditory menu, gestural interaction},
	pages = {1395–1404},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/EGVXE3D4/Zhao et al. - 2007 - Earpod Eyes-free Menu Selection Using Touch Input.pdf:application/pdf}
},

@inproceedings{vspakov_enhanced_2012,
	address = {New York, {NY}, {USA}},
	series = {{UbiComp} '12},
	title = {Enhanced Gaze Interaction Using Simple Head Gestures},
	isbn = {978-1-4503-1224-0},
	url = {http://doi.acm.org/10.1145/2370216.2370369},
	doi = {10.1145/2370216.2370369},
	abstract = {We propose a combination of gaze pointing and head gestures for enhanced hands-free interaction. Instead of the traditional dwell-time selection method, we experimented with five simple head gestures: nodding, turning left/right, and tilting left/right. The gestures were detected from the eye-tracking data by a range-based algorithm, which was found accurate enough in recognizing nodding and left-directed gestures. The gaze estimation accuracy did not noticeably suffer from the quick head motions. Participants pointed to nodding as the best gesture for occasional selections tasks and rated the other gestures as promising methods for navigation (turning) and functional mode switching (tilting). In general, dwell time works well for repeated tasks such as eye typing. However, considering multimodal games or transient interactions in pervasive and mobile environments, we believe a combination of gaze and head interaction could potentially provide a natural and more accurate interaction method.},
	urldate = {2014-01-06},
	booktitle = {Proceedings of the 2012 {ACM} Conference on Ubiquitous Computing},
	publisher = {{ACM}},
	author = {{\textbackslash}{vSpakov}, Oleg and Majaranta, Päivi},
	year = {2012},
	keywords = {dwell time, eye tracking, head gestures, selection},
	pages = {705–710},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/R73USQIH/vSpakov and Majaranta - 2012 - Enhanced Gaze Interaction Using Simple Head Gestur.pdf:application/pdf}
},

@article{derry_evaluating_2009,
	title = {Evaluating Head Gestures for Panning 2-D Spatial Information},
	url = {http://digitalcommons.calpoly.edu/cgi/viewcontent.cgi?article=1207&context=theses},
	urldate = {2013-11-13},
	journal = {Master's Theses and Project Reports},
	author = {Derry, Matthew O.},
	year = {2009},
	pages = {193},
	file = {Evaluating Head Gestures for Panning 2-D Spatial Information.pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/G6297T55/Evaluating Head Gestures for Panning 2-D Spatial Information.pdf:application/pdf}
},

@inproceedings{yi_exploring_2012,
	address = {New York, {NY}, {USA}},
	series = {{CHI} '12},
	title = {Exploring User Motivations for Eyes-free Interaction on Mobile Devices},
	isbn = {978-1-4503-1015-4},
	url = {http://doi.acm.org/10.1145/2207676.2208678},
	doi = {10.1145/2207676.2208678},
	abstract = {While there is increasing interest in creating eyes-free interaction technologies, a solid analysis of why users need or desire eyes-free interaction has yet to be presented. To gain a better understanding of such user motivations, we conducted an exploratory study with four focus groups, and suggest a classification of motivations for eyes-free interaction under four categories (environmental, social, device features, and personal). Exploring and analyzing these categories, we present early insights pointing to design implications for future eyes-free interactions.},
	urldate = {2014-02-03},
	booktitle = {Proceedings of the {SIGCHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Yi, Bo and Cao, Xiang and Fjeld, Morten and Zhao, Shengdong},
	year = {2012},
	keywords = {eyes-free, mobile devices, user motivation},
	pages = {2789–2792},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/WG3BQ7T2/Yi et al. - 2012 - Exploring User Motivations for Eyes-free Interacti.pdf:application/pdf}
},

@inproceedings{mardanbegi_eye-based_2012,
	address = {New York, {NY}, {USA}},
	series = {{ETRA} '12},
	title = {Eye-based Head Gestures},
	isbn = {978-1-4503-1221-9},
	url = {http://doi.acm.org/10.1145/2168556.2168578},
	doi = {10.1145/2168556.2168578},
	abstract = {A novel method for video-based head gesture recognition using eye information by an eye tracker has been proposed. The method uses a combination of gaze and eye movement to infer head gestures. Compared to other gesture-based methods a major advantage of the method is that the user keeps the gaze on the interaction object while interacting. This method has been implemented on a head-mounted eye tracker for detecting a set of predefined head gestures. The accuracy of the gesture classifier is evaluated and verified for gaze-based interaction in applications intended for both large public displays and small mobile phone screens. The user study shows that the method detects a set of defined gestures reliably.},
	urldate = {2014-01-06},
	booktitle = {Proceedings of the Symposium on Eye Tracking Research and Applications},
	publisher = {{ACM}},
	author = {Mardanbegi, Diako and Hansen, Dan Witzner and Pederson, Thomas},
	year = {2012},
	keywords = {eye tracker, gaze interaction, head gestures, interaction},
	pages = {139–146},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/SHIZ969U/Mardanbegi et al. - 2012 - Eye-based Head Gestures.pdf:application/pdf}
},

@article{kajastila_eyes-free_2013,
	title = {Eyes-free interaction with free-hand gestures and auditory menus},
	volume = {71},
	issn = {1071-5819},
	url = {http://www.sciencedirect.com/science/article/pii/S1071581912002042},
	doi = {10.1016/j.ijhcs.2012.11.003},
	abstract = {Auditory interfaces can overcome visual interfaces when a primary task, such as driving, competes for the attention of a user controlling a device, such as radio. In emerging interfaces enabled by camera tracking, auditory displays may also provide viable alternatives to visual displays. This paper presents a user study of interoperable auditory and visual menus, in which control gestures remain the same in the visual and the auditory domain. Tested control methods included a novel free-hand gesture interaction with camera-based tracking, and touch screen interaction with a tablet. The task of the participants was to select numbers from a visual or an auditory menu including a circular layout and a numeric keypad layout. Results show, that even with participant's full attention to the task, the performance and accuracy of the auditory interface are the same or even slightly better than the visual when controlled with free-hand gestures. The auditory menu was measured to be slower in touch screen interaction, but questionnaire revealed that over half of the participants felt that the circular auditory menu was faster than the visual menu. Furthermore, visual and auditory feedback in touch screen interaction with numeric layout was measured fastest, touch screen with circular menu second fastest, and the free-hand gesture interface was slowest. The results suggest that auditory menus can potentially provide a fast and desirable interface to control devices with free-hand gestures.},
	number = {5},
	urldate = {2014-01-27},
	journal = {International Journal of Human-Computer Studies},
	author = {Kajastila, Raine and Lokki, Tapio},
	month = may,
	year = {2013},
	keywords = {Auditory display, Auditory menu, Eyes free, Free-hand gestures, interaction, Spatial sound},
	pages = {627--640},
	file = {ScienceDirect Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/7EWVSC9Q/Kajastila and Lokki - 2013 - Eyes-free interaction with free-hand gestures and .pdf:application/pdf;ScienceDirect Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/JQN56T7H/S1071581912002042.html:text/html}
},

@inproceedings{vazquez-alvarez_eyes-free_2011,
	title = {Eyes-free multitasking: the effect of cognitive load on mobile spatial audio interfaces},
	shorttitle = {Eyes-free multitasking},
	url = {http://dl.acm.org/citation.cfm?id=1979258},
	urldate = {2014-01-07},
	booktitle = {Proceedings of the {SIGCHI} Conference on Human Factors in Computing Systems},
	author = {Vazquez-Alvarez, Yolanda and Brewster, Stephen A.},
	year = {2011},
	pages = {2173–2176},
	file = {CHI2011_yolanda.pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/BRH6U4F9/CHI2011_yolanda.pdf:application/pdf}
},

@book{barfield_fundamentals_2000,
	address = {Hillsdale, {NJ}, {USA}},
	title = {Fundamentals of Wearable Computers and Augumented Reality},
	isbn = {0805829024},
	publisher = {L. Erlbaum Associates Inc.},
	author = {Barfield, Woodrow and Caudell, Thomas},
	year = {2000}
},

@incollection{park_gaze-directed_2011,
	series = {Lecture Notes in Computer Science},
	title = {Gaze-Directed Hands-Free Interface for Mobile Interaction},
	copyright = {©2011 Springer-Verlag {GmbH} Berlin Heidelberg},
	isbn = {978-3-642-21604-6, 978-3-642-21605-3},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-21605-3_34},
	abstract = {While mobile devices have allowed people to carry out various computing and communication tasks everywhere, it has generally lacked the support for task execution while the user is in motion. This is because the interaction schemes of most mobile applications are centered around the device visual display and when in motion (with the important body parts, such as the head and hands, moving), it is difficult for the user to recognize the visual output on the small hand-carried device display and respond to make the timely and proper input. In this paper, we propose an interface which allows the user to interact with the mobile devices during motion without having to look at it or use one’s hands. More specifically, the user interacts, by gaze and head motion gestures, with an invisible virtual interface panel with the help of a head-worn gyro sensor and aural feedback. Since the menu is one of the most prevailing methods of interaction, we investigate and focus on the various forms of menu presentation such as the layout and the number of comfortably selectable menu items. With head motion, it turns out 4×2 or 3×3 grid menu is more effective. The results of this study can be further extended for developing a more sophisticated non-visual oriented mobile interface.},
	number = {6762},
	urldate = {2014-02-06},
	booktitle = {Human-Computer Interaction. Interaction Techniques and Environments},
	publisher = {Springer Berlin Heidelberg},
	author = {Park, Gie-seo and Ahn, Jong-gil and Kim, Gerard J.},
	editor = {Jacko, Julie A.},
	month = jan,
	year = {2011},
	keywords = {Data Mining and Knowledge Discovery, Gaze, Hands-free, Head-controlled, Image Processing and Computer Vision, Information Systems Applications ({incl.Internet)}, Mobile interface, Multimedia Information Systems, Non-visual interface, Signal, Image and Speech Processing, User Interfaces and Human Computer Interaction},
	pages = {304--313},
	file = {Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/HKX25ZPX/Park et al. - 2011 - Gaze-Directed Hands-Free Interface for Mobile Inte.pdf:application/pdf;Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/Q8QCSIT6/10.html:text/html}
},

@inproceedings{pirhonen_gestural_2002,
	address = {New York, {NY}, {USA}},
	series = {{CHI} '02},
	title = {Gestural and Audio Metaphors As a Means of Control for Mobile Devices},
	isbn = {1-58113-453-3},
	url = {http://doi.acm.org/10.1145/503376.503428},
	doi = {10.1145/503376.503428},
	abstract = {This paper discusses the use of gesture and non-speech audio as ways to improve the user interface of a mobile music player. Their key advantages mean that users could use a player without having to look at its controls when on the move. Two very different evaluations of the player took place: one based on a standard usability experiment (comparing the new player to a standard design) and the other a video analysis of the player in use. Both of these showed significant usability improvements for the gesture/audio-based interface over a standard visual/pen-based display. The similarities and differences in the results produced by the two studies are discussed},
	urldate = {2014-01-27},
	booktitle = {Proceedings of the {SIGCHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Pirhonen, Antti and Brewster, Stephen and Holguin, Christopher},
	year = {2002},
	keywords = {evaluation, gestures, metaphor, mobile computing, non-speech audio},
	pages = {291–298},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/ECWHI8PK/Pirhonen et al. - 2002 - Gestural and Audio Metaphors As a Means of Control.pdf:application/pdf}
},

@book{mackay_hci_1997,
	title = {{HCI}, Natural Science and Design: A Framework for Triangulation Across Disciplines},
	shorttitle = {{HCI}, Natural Science and Design},
	abstract = {Human-computer interaction is multidisciplinary, drawing paradigms and techniques from both the natural sciences and the design disciplines. {HCI} cannot be considered a pure natural science because it studies the interaction between people and artificially-created artifacts, rather than naturally-occurring phenomena, which violates several basic assumptions of natural science. Similarly, {HCI} cannot be considered a pure design discipline because it strives to independently verify design decisions and processes, and borrows many values from scientists.  The purpose of this paper is to provide a simple framework that describes how the research and design models underlying {HCI} can be integrated. We explore the relationships among these approaches in the context of a particular research site, {CENA}, the Centre d' tudes de la Navigation Arienne, and illustrate how the various disciplines can contribute to a complex design problem: improving the interface to the French air traffic control syste...},
	author = {Mackay, Wendy E. and Fayard, Anne-Laure},
	year = {1997},
	file = {Citeseer - Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/PEMU69HZ/Mackay and Fayard - 1997 - HCI, Natural Science and Design A Framework for T.pdf:application/pdf;Citeseer - Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/M9GMURXT/summary.html:text/html}
},

@inproceedings{hermann_head_2012,
	address = {New York, {NY}, {USA}},
	series = {{AM} '12},
	title = {Head Gesture Sonification for Supporting Social Interaction},
	isbn = {978-1-4503-1569-2},
	url = {http://doi.acm.org/10.1145/2371456.2371469},
	doi = {10.1145/2371456.2371469},
	abstract = {In this paper we introduce two new methods for real-time sonification of head movements and head gestures. Head gestures such as nodding or shaking the head are important non-verbal back-channelling signals which facilitate coordination and alignment of communicating interaction partners. Visually impaired persons cannot interpret such non-verbal signals, same as people in mediated communication (e.g. on the phone), or cooperating users whose visual attention is focused elsewhere. We introduce our approach to tackle these issues, our sensing setup and two different sonification methods. A first preliminary study on the recognition of signals shows that subjects understand the gesture type even without prior explanation and can estimate gesture intensity and frequency with no or little training.},
	urldate = {2014-02-06},
	booktitle = {Proceedings of the 7th Audio Mostly Conference: A Conference on Interaction with Sound},
	publisher = {{ACM}},
	author = {Hermann, Thomas and Neumann, Alexander and Zehe, Sebastian},
	year = {2012},
	keywords = {assistive technology, Auditory display, head gestures, interaction technology, mediated communication, social interaction, sonification},
	pages = {82–89},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/9E3CFKTP/Hermann et al. - 2012 - Head Gesture Sonification for Supporting Social In.pdf:application/pdf}
},

@inproceedings{lu_head_2005,
	title = {Head nod and shake recognition based on multi-view model and hidden Markov model},
	doi = {10.1109/CGIV.2005.41},
	abstract = {Head gestures such as nodding and shaking are often used as one of human body languages for communication with each other, and their recognition plays an important role in the advancement of human-computer interaction. As head gesture is the continuous motion on the sequential time series, the key problems of recognition are to track multi-view head and understand the head pose switch. This paper presents a novel approach to recognize the nod and shake in the interactive computer environment. First, head poses are detected by multi-view model ({MVM)} and then hidden Markov model ({HMM)} are used as head gesture statistic inference model for gesture recognition. Experimental results show that the approach is effective and real time.},
	booktitle = {International Conference on Computer Graphics, Imaging and Vision: New Trends, 2005},
	author = {Lu, Peng and Zhang, Mandun and Zhu, Xinshan and Wang, Yangsheng},
	year = {2005},
	keywords = {Biological system modeling, Detectors, gesture recognition, Hardware, Head, head gesture statistic inference model, head nod recognition, head pose switch, head shake recognition, hidden Markov model, hidden Markov models, {HMM}, human body language, human computer interaction, human-computer interaction, interactive computer environment, Maximum likelihood detection, multiview model, sequential time series, Statistics, Switches, time series, Tracking},
	pages = {61--64},
	file = {IEEE Xplore Abstract Record:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/4S8K5HSD/articleDetails.html:text/html}
},

@inproceedings{malkewitz_head_1998,
	address = {New York, {NY}, {USA}},
	series = {Assets '98},
	title = {Head Pointing and Speech Control As a Hands-free Interface to Desktop Computing},
	isbn = {1-58113-020-1},
	url = {http://doi.acm.org/10.1145/274497.274531},
	doi = {10.1145/274497.274531},
	urldate = {2014-01-27},
	booktitle = {Proceedings of the Third International {ACM} Conference on Assistive Technologies},
	publisher = {{ACM}},
	author = {Malkewitz, Rainer},
	year = {1998},
	keywords = {disabled users, head gestures, pointing device, speech input},
	pages = {182–188},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/9UXW6INM/Malkewitz - 1998 - Head Pointing and Speech Control As a Hands-free I.pdf:application/pdf}
},

@inproceedings{klemmer_how_2006,
	title = {How Bodies Matter: Five Themes for Interaction Design},
	shorttitle = {How Bodies Matter},
	abstract = {Our physical bodies play a central role in shaping human experience in the world, understanding of the world, and interactions in the world. This paper draws on theories of embodiment — from psychology, sociology, and philosophy — synthesizing five themes we believe are particularly salient for interaction design: thinking through doing, performance, visibility, risk, and thick practice. We introduce aspects of human embodied engagement in the world with the goal of inspiring new interaction design approaches and evaluations that better integrate the physical and computational worlds. Author Keywords Embodiment, bodies, embodied interaction, ubiquitous},
	booktitle = {Carnegie Mellon University},
	publisher = {Press},
	author = {Klemmer, Scott R. and Hartmann, Björn},
	year = {2006},
	pages = {140–149},
	file = {Citeseer - Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/WWKDA9K7/Klemmer and Hartmann - 2006 - How Bodies Matter Five Themes for Interaction Des.pdf:application/pdf;Citeseer - Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/FZF7CJWD/summary.html:text/html}
},

@inproceedings{klemmer_how_2006-1,
	address = {New York, {NY}, {USA}},
	series = {{DIS} '06},
	title = {How Bodies Matter: Five Themes for Interaction Design},
	isbn = {1-59593-367-0},
	shorttitle = {How Bodies Matter},
	url = {http://doi.acm.org/10.1145/1142405.1142429},
	doi = {10.1145/1142405.1142429},
	abstract = {Our physical bodies play a central role in shaping human experience in the world, understandingof the world, and  interactions in the world. This paper draws on theories of embodiment - from psychology, sociology, and philosophy - synthesizing five themes we believe are particularly salient for interaction design: thinking through doing, performance, visibility, risk, and thick practice. We intro-duce aspects of human embodied engagement in the world with the goal of inspiring new interaction design ap-proaches and evaluations that better integrate the physical and computational worlds.},
	urldate = {2014-04-13},
	booktitle = {Proceedings of the 6th Conference on Designing Interactive Systems},
	publisher = {{ACM}},
	author = {Klemmer, Scott R. and Hartmann, Björn and Takayama, Leila},
	year = {2006},
	keywords = {bodies, embodied interaction, embodiment, interaction design, phenomenology, ubiquitous computing},
	pages = {140–149},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/844PVCAP/Klemmer et al. - 2006 - How Bodies Matter Five Themes for Interaction Des.pdf:application/pdf}
},

@book{meinard_muller_information_2007,
	title = {Information Retrieval for Music and Motion},
	author = {{Meinard Müller}},
	year = {2007},
	file = {9783540740476-c1 (2).pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/MNIX7APC/9783540740476-c1 (2).pdf:application/pdf}
},

@inproceedings{vassiliou_integrated_2000,
	title = {Integrated multimodal human-computer interface and augmented reality for interactive display applications},
	volume = {4022},
	url = {http://dx.doi.org/10.1117/12.397779},
	doi = {10.1117/12.397779},
	abstract = {We describe new systems for improved integrated multimodal human-computer interaction and augmented reality for a diverse array of applications, including future advanced cockpits, tactical operations centers, and others. We have developed an integrated display system featuring: speech recognition of multiple concurrent users equipped with both standard air- coupled microphones and novel throat-coupled sensors (developed at Army Research Labs for increased noise immunity); lip reading for improving speech recognition accuracy in noisy environments, three-dimensional spatialized audio for improved display of warnings, alerts, and other information; wireless, coordinated handheld-{PC} control of a large display; real-time display of data and inferences from wireless integrated networked sensors with on-board signal processing and discrimination; gesture control with disambiguated point-and-speak capability; head- and eye- tracking coupled with speech recognition for 'look-and-speak' interaction; and integrated tetherless augmented reality on a wearable computer. The various interaction modalities (speech recognition, {3D} audio, eyetracking, etc.) are implemented a 'modality servers' in an Internet-based client-server architecture. Each modality server encapsulates and exposes commercial and research software packages, presenting a socket network interface that is abstracted to a high-level interface, minimizing both vendor dependencies and required changes on the client side as the server's technology improves.},
	urldate = {2014-03-18},
	author = {Vassiliou, Marius S. and Sundareswaran, Venkataraman and Chen, S. and Behringer, Reinhold and Tam, Clement K. and Chan, M. and Bangayan, Phil T. and {McGee}, Joshua H.},
	year = {2000},
	pages = {106--115}
},

@inproceedings{taib_integrating_2008,
	address = {Berlin, Heidelberg},
	series = {{MLMI'07}},
	title = {Integrating Semantics into Multimodal Interaction Patterns},
	isbn = {3-540-78154-4, 978-3-540-78154-7},
	url = {http://dl.acm.org/citation.cfm?id=1787422.1787434},
	abstract = {A user experiment on multimodal interaction (speech, hand position and hand shapes) to study two major relationships: between the level of cognitive load experienced by users and the resulting multimodal interaction patterns; and how the semantics of the information being conveyed affected those patterns. We found that as cognitive load increases, users' multimodal productions tend to become semantically more complementary and less redundant across modalities. This validates cognitive load theory as a theoretical background for understanding the occurrence of particular kinds of multimodal productions. Moreover, results indicate a significant relationship between the temporal multimodal integration pattern (7 patterns in this experiment) and the semantics of the command being issued by the user (4 types of commands), shedding new light on previous research findings that assign a unique temporal integration pattern to any given subject regardless of the communication taking place.},
	urldate = {2014-01-06},
	booktitle = {Proceedings of the 4th International Conference on Machine Learning for Multimodal Interaction},
	publisher = {Springer-Verlag},
	author = {Taib, Ronnie and Ruiz, Natalie},
	year = {2008},
	pages = {96–107}
},

@misc{gn_store_nord_intelligent_2013,
	title = {Intelligent Headset™ {\textbar} The worlds first Intelligent Headset™},
	url = {https://intelligentheadset.com/},
	urldate = {2014-03-26},
	author = {{{GN} Store Nord}},
	year = {2013},
	file = {Intelligent Headset™ | The worlds first Intelligent Headset™:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/U4EFE7E7/intelligentheadset.com.html:text/html}
},

@article{billinghurst_interaction_????,
	title = {Interaction on the Go},
	url = {http://www.ep.liu.se/ecp/011/001/ecp011001b.pdf},
	urldate = {2014-03-07},
	author = {Billinghurst, Mark},
	file = {ecp011001b.pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/A9JZ3HEG/ecp011001b.pdf:application/pdf}
},

@article{kajastila_interaction_2013,
	title = {Interaction with eyes-free and gestural interfaces},
	url = {https://aaltodoc.aalto.fi/handle/123456789/7720},
	urldate = {2014-01-07},
	author = {Kajastila, Raine},
	year = {2013},
	file = {isbn9789526050034.pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/483GFBGQ/isbn9789526050034.pdf:application/pdf}
},

@incollection{sweller_measuring_2011,
	series = {Explorations in the Learning Sciences, Instructional Systems and Performance Technologies},
	title = {Measuring Cognitive Load},
	copyright = {©2011 Springer {Science+Business} Media, {LLC}},
	isbn = {978-1-4419-8125-7, 978-1-4419-8126-4},
	url = {http://link.springer.com/chapter/10.1007/978-1-4419-8126-4_6},
	abstract = {Because of the centrality of working memory load to cognitive load theory, measuring this load has been a high priority for researchers. While it is possible to demonstrate support for the validity of the theory by predicting experimental outcomes, it is useful to additionally provide independent measures of cognitive load. In this chapter we describe the various methods used to measure cognitive load and how they have developed over the last 30 years.},
	language = {en},
	number = {1},
	urldate = {2014-02-03},
	booktitle = {Cognitive Load Theory},
	publisher = {Springer New York},
	author = {Sweller, John and Ayres, Paul and Kalyuga, Slava},
	month = jan,
	year = {2011},
	keywords = {Cognitive Psychology, Educational Psychology, Learning \& Instruction},
	pages = {71--85},
	file = {Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/RFPRKA4Q/Sweller et al. - 2011 - Measuring Cognitive Load.pdf:application/pdf;Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/R6TCFR62/10.html:text/html}
},

@article{kjeldskov_mobile_2013,
	title = {Mobile Computing},
	url = {/encyclopedia/mobile_computing.html},
	abstract = {A visual history of mobile computing...},
	urldate = {2014-03-07},
	journal = {The Encyclopedia of Human-Computer Interaction, 2nd Ed.},
	author = {Kjeldskov, Jesper},
	year = {2013}
},

@inproceedings{marshall_mobile_2013,
	address = {New York, {NY}, {USA}},
	series = {{CHI} {EA} '13},
	title = {Mobile Interaction Does Not Exist},
	isbn = {978-1-4503-1952-2},
	url = {http://doi.acm.org/10.1145/2468356.2468725},
	doi = {10.1145/2468356.2468725},
	abstract = {Most mobile systems are 'stop-to-interact'; designed for active interaction only when a user is standing still, paying visual and mental attention to the device. However, people are increasingly carrying and using devices while undertaking a wide range of movement activities, such as walking, cycling, running. Some existing systems such as Apple's Siri aim for hands and eyes free use, but they do not consider the wider challenges of interaction during movement. We describe the challenges of system design for active mobile interaction. These 'interaction in motion' challenges are discussed with reference to an extreme movement interaction situation - cold water swimming.},
	urldate = {2014-03-07},
	booktitle = {{CHI} '13 Extended Abstracts on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Marshall, Joe and Tennent, Paul},
	year = {2013},
	keywords = {interaction, interaction in motion, mobile, motion, swimming},
	pages = {2069–2078},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/UVV3K5B8/Marshall and Tennent - 2013 - Mobile Interaction Does Not Exist.pdf:application/pdf}
},

@article{cohen_multidimensional_1991,
	title = {Multidimensional audio window management},
	volume = {34},
	issn = {0020-7373},
	url = {http://www.sciencedirect.com/science/article/pii/002073739190023Z},
	doi = {10.1016/0020-7373(91)90023-Z},
	abstract = {This paper proposes and organization of presentation and control that implements a flexible audio management system we call “audio windows”. The result is a new user interface integrating and enhanced spatial sound presentation system, an audio emphasis system, and a gestural input recognition system. We have implemented these ideas in a modest prototype, also described, designed as an audio server appropriate for a teleconferencing system. Our system combines a gestural front end (currently based on a {DataGlove}, but whose concepts are appropriate for other devices as well) with an enhanced spatial sound system, a digital signal processing separation of multiple sound sources, augmented with “filtears”, audio feedback cues that convey added information without distraction or loss of intelligibility. Our prototype employs a manual front end (requiring no keyboard or mouse) driving an auditory back end (requiring no {CRT} or visual display).},
	number = {3},
	urldate = {2014-01-27},
	journal = {International Journal of Man-Machine Studies},
	author = {Cohen, Michael and Ludwig, Lester F.},
	month = mar,
	year = {1991},
	pages = {319--336},
	file = {ScienceDirect Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/D3JGTQWG/Cohen and Ludwig - 1991 - Multidimensional audio window management.pdf:application/pdf;ScienceDirect Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/B76EG7IE/002073739190023Z.html:text/html}
},

@article{jaimes_multimodal_2007,
	title = {Multimodal human–computer interaction: A survey},
	volume = {108},
	issn = {1077-3142},
	shorttitle = {Multimodal human–computer interaction},
	url = {http://www.sciencedirect.com/science/article/pii/S1077314206002335},
	doi = {10.1016/j.cviu.2006.10.019},
	abstract = {In this paper, we review the major approaches to multimodal human–computer interaction, giving an overview of the field from a computer vision perspective. In particular, we focus on body, gesture, gaze, and affective interaction (facial expression recognition and emotion in audio). We discuss user and task modeling, and multimodal fusion, highlighting challenges, open issues, and emerging applications for multimodal human–computer interaction ({MMHCI)} research.},
	number = {1–2},
	urldate = {2014-03-18},
	journal = {Computer Vision and Image Understanding},
	author = {Jaimes, Alejandro and Sebe, Nicu},
	month = oct,
	year = {2007},
	keywords = {Affective computing, gesture recognition, Human-centered computing, Multimodal human-computer interaction},
	pages = {116--134},
	file = {ScienceDirect Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/8QUT9D3R/Jaimes and Sebe - 2007 - Multimodal human–computer interaction A survey.pdf:application/pdf;ScienceDirect Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/MFPFBIEF/S1077314206002335.html:text/html}
},

@book{beskow_multimodal_????,
	title = {Multimodal Interaction Control},
	abstract = {No matter how well hidden our systems are and how well they do their magic unnoticed in the background, there are times when direct interaction between system and human is a necessity. As long as the interaction can take place unobtrusively and without techno-clutter, this is desirable. It is hard to picture a means of interaction less obtrusive and techno-cluttered than spoken communication on human terms. Spoken face-to-face communication is the most intuitive and robust form of communication between humans imaginable. In order to exploit such human spoken communication to its full potential as an interface between human and machine, we need a much better understanding of how the more human-like aspects of spoken communication work. A crucial aspect of face-to-face conversation is what people do and what they take into consideration in order to manage the flow of the interaction. For example, participants in a conversation have to be able to identify places where it is legitimate to begin to talk, as well as to avoid interrupting their interlocutors. The ability to indicate that you want to say something, that somebody else may start talking, or},
	author = {Beskow, Jonas and Carlson, Rolf and Edlund, Jens and Granström, Björn and Heldner, Mattias and Hjalmarsson, Anna and Skantze, Gabriel},
	file = {Citeseer - Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/PJPGGDUE/Beskow et al. - Multimodal Interaction Control.pdf:application/pdf;Citeseer - Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/VWRFBQET/summary.html:text/html}
},

@book{tzovaras_dimitrios_multimodal_2008,
	title = {Multimodal User Interfaces},
	author = {{Tzovaras, Dimitrios}},
	year = {2008}
},

@inproceedings{brewster_multimodaleyes-freeinteraction_2003,
	title = {Multimodal'eyes-free'interaction techniques for wearable devices},
	url = {http://dl.acm.org/citation.cfm?id=642694},
	urldate = {2014-01-07},
	booktitle = {Proceedings of the {SIGCHI} conference on Human factors in computing systems},
	author = {Brewster, Stephen and Lumsden, Joanna and Bell, Marek and Hall, Malcolm and Tasker, Stuart},
	year = {2003},
	keywords = {{FINAL}, {RELATED} {SYSTEM}},
	pages = {473–480},
	file = {CHI2003.pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/D7JHEUEX/CHI2003.pdf:application/pdf}
},

@misc{emarketer_music_2014,
	title = {Music Goes Mobile as More Smartphone Users Stream Songs},
	url = {http://www.emarketer.com/Article/Music-Goes-Mobile-More-Smartphone-Users-Stream-Songs/1010126},
	abstract = {New estimates from {eMarketer} find that more than one out of five Americans will listen to music on their phones this year. Nearly all mobile music listening will take place on smartphones, which will be in the hands of 99.2\% of total mobile music listeners.},
	urldate = {2014-03-11},
	author = {{{eMarketer}}},
	year = {2014},
	file = {Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/XMB7R6EV/1010126.html:text/html}
},

@inproceedings{lopresti_neck_2000,
	address = {New York, {NY}, {USA}},
	series = {Assets '00},
	title = {Neck Range of Motion and Use of Computer Head Controls},
	isbn = {1-58113-313-8},
	url = {http://doi.acm.org/10.1145/354324.354352},
	doi = {10.1145/354324.354352},
	urldate = {2014-02-11},
	booktitle = {Proceedings of the Fourth International {ACM} Conference on Assistive Technologies},
	publisher = {{ACM}},
	author = {{LoPresti}, Edmund and Brienza, David M. and Angelo, Jennifer and Gilbertson, Lars and Sakai, Jonathan},
	year = {2000},
	keywords = {disability, head controls, head movement},
	pages = {121–128},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/N8CGTX6S/LoPresti et al. - 2000 - Neck Range of Motion and Use of Computer Head Cont.pdf:application/pdf}
},

@incollection{bonner_no-look_2010,
	series = {Lecture Notes in Computer Science},
	title = {No-Look Notes: Accessible Eyes-Free Multi-touch Text Entry},
	copyright = {©2010 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-12653-6, 978-3-642-12654-3},
	shorttitle = {No-Look Notes},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-12654-3_24},
	abstract = {Mobile devices with multi-touch capabilities are becoming increasingly common, largely due to the success of the Apple {iPhone} and {iPod} Touch. While there have been some advances in touchscreen accessibility for blind people, touchscreens remain inaccessible in many ways. Recent research has demonstrated that there is great potential in leveraging multi-touch capabilities to increase the accessibility of touchscreen applications for blind people. We have created No-Look Notes, an eyes-free text entry system that uses multi-touch input and audio output. No-Look Notes was implemented on Apple’s {iPhone} platform. We have performed a within-subjects (n = 10) user study of both No-Look Notes and the text entry component of Apple’s {VoiceOver}, the recently released official accessibility component on the {iPhone.} No-Look Notes significantly outperformed {VoiceOver} in terms of speed, accuracy and user preference.},
	number = {6030},
	urldate = {2014-02-03},
	booktitle = {Pervasive Computing},
	publisher = {Springer Berlin Heidelberg},
	author = {Bonner, Matthew N. and Brudvik, Jeremy T. and Abowd, Gregory D. and Edwards, W. Keith},
	editor = {Floréen, Patrik and Krüger, Antonio and Spasojevic, Mirjana},
	month = jan,
	year = {2010},
	keywords = {accessibility, Computer Communication Networks, eyes-free, Information Storage and Retrieval, Information Systems Applications ({incl.Internet)}, mobile device, multi-touch, Operating Systems, Software Engineering, text entry, touchscreen, User Interfaces and Human Computer Interaction},
	pages = {409--426},
	file = {Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/ANZP5Z5E/Bonner et al. - 2010 - No-Look Notes Accessible Eyes-Free Multi-touch Te.pdf:application/pdf;Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/W9SZ95GW/10.html:text/html}
},

@article{kleinrock_nomadic_1995,
	title = {Nomadic Computing - an Opportunity},
	volume = {25},
	issn = {0146-4833},
	url = {http://doi.acm.org/10.1145/205447.205450},
	doi = {10.1145/205447.205450},
	abstract = {We are in the midst of some truly revolutionary changes in the field of computer-communications, and these offer opportunities and challenges to the research community. One of these changes has to do with nomadic computing and communications. Nomadicity refers to the system support needed to provide a rich set of capabilities and services to the nomad as he moves from place to place in a transparent and convenient form. This new paradigm is already manifesting itself as users travel to many different locations with laptops, {PDA's}, cellular telephones, pagers, etc. In this paper we discuss some of the open issues that must be addressed as we bring about the system support necessary for nomadicity. In addition, we present some of the considerations with which one must be concerned in the area of wireless communications, which forms one (and only one) component of nomadicity.},
	number = {1},
	urldate = {2014-03-07},
	journal = {{SIGCOMM} Comput. Commun. Rev.},
	author = {Kleinrock, Leonard},
	month = jan,
	year = {1995},
	pages = {36–40},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/G6UTTGUB/Kleinrock - 1995 - Nomadic Computing&Mdash;an Opportunity.pdf:application/pdf}
},

@article{sawhney_nomadic_2000,
	title = {Nomadic radio: speech and audio interaction for contextual messaging in nomadic environments},
	volume = {7},
	shorttitle = {Nomadic radio},
	url = {http://dl.acm.org/citation.cfm?id=355327},
	number = {3},
	urldate = {2014-01-07},
	journal = {{ACM} transactions on Computer-Human interaction ({TOCHI)}},
	author = {Sawhney, Nitin and Schmandt, Chris},
	year = {2000},
	pages = {353–383},
	file = {sawhney_ToCHI00_nomadic_radio.pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/QXFZ67DZ/sawhney_ToCHI00_nomadic_radio.pdf:application/pdf}
},

@misc{_online_????,
	title = {Online music streaming doubled in 2013 - stats},
	url = {http://econsultancy.com/blog/64066-online-music-streaming-doubled-in-2013-stats?utm_campaign=bloglikes&utm_medium=socialnetwork&utm_source=facebook},
	abstract = {In 2013, 7.4bn songs were streamed in the {UK}, doubling the previous year's total of 3.7bn.},
	urldate = {2014-03-07},
	journal = {Econsultancy},
	file = {Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/2CGEZSQP/64066-online-music-streaming-doubled-in-2013-stats.html:text/html}
},

@misc{_online_????-1,
	title = {Online music streaming doubled in 2013 - stats},
	url = {http://econsultancy.com/blog/64066-online-music-streaming-doubled-in-2013-stats?utm_campaign=bloglikes&utm_medium=socialnetwork&utm_source=facebook},
	abstract = {In 2013, 7.4bn songs were streamed in the {UK}, doubling the previous year's total of 3.7bn.},
	urldate = {2014-03-07},
	journal = {Econsultancy},
	file = {Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/CSX3ZQXK/64066-online-music-streaming-doubled-in-2013-stats.html:text/html}
},

@article{brewster_overcoming_2002,
	title = {Overcoming the lack of screen space on mobile computers},
	volume = {6},
	url = {http://dl.acm.org/citation.cfm?id=594356},
	number = {3},
	urldate = {2014-01-07},
	journal = {Personal and Ubiquitous Computing},
	author = {Brewster, Stephen},
	year = {2002},
	pages = {188–205},
	file = {overcoming1.pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/RCFTSVKT/overcoming1.pdf:application/pdf}
},

@article{roth_patterns_2002,
	title = {Patterns of Mobile Interaction},
	volume = {6},
	issn = {1617-4909},
	url = {http://dx.doi.org/10.1007/s007790200029},
	doi = {10.1007/s007790200029},
	abstract = {The design of systems for mobile scenarios covers a wide range of issues, ranging from mobile networking to user interface design for mobile devices. Mobile applications often run distributed on several connected devices, used by many users simultaneously. Considering all issues related to mobile scenarios, a designer might be overwhelmed. As a solution, we propose a specific kind of design patterns which we call mobility patterns, derived from successful mobile applications. They allow a designer to re-use design elements as building blocks in their own designs. After describing the idea of mobility patterns, we give a brief overview of patterns we have identified so far. Two patterns are described in more detail with the help of our research platforms {QuickStep} and Pocket {DreamTeam.}},
	number = {4},
	urldate = {2014-03-07},
	journal = {Personal Ubiquitous Comput.},
	author = {Roth, Jörg},
	month = jan,
	year = {2002},
	keywords = {Application design, Design patterns, mobile computing, Mobile interaction},
	pages = {282–289},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/GK7WCNIF/Roth - 2002 - Patterns of Mobile Interaction.pdf:application/pdf}
},

@inproceedings{pielot_pocketmenu:_2012,
	address = {New York, {NY}, {USA}},
	series = {{MobileHCI} '12},
	title = {{PocketMenu:} Non-visual Menus for Touch Screen Devices},
	isbn = {978-1-4503-1105-2},
	shorttitle = {{PocketMenu}},
	url = {http://doi.acm.org/10.1145/2371574.2371624},
	doi = {10.1145/2371574.2371624},
	abstract = {We present {PocketMenu}, a menu optimized for non-visual, in-pocket interaction with menus on handheld devices with touch screens. By laying out menu items along the border of the touch screen its tactile features guide the interaction. Additional vibro-tactile feedback and speech allows identifying the individual menu items non-visually. In an experiment, we compared {PocketMenu} with {iPhone's} {VoiceOver.} Participants had to control an {MP3} player while walking down a road with the device in the pocket. The results provide evidence that in the above context the {PocketMenu} outperforms {VoiceOver} in terms of completion time, selection errors, usability. Hence, it enables usage of touch screen apps in mobile contexts (e.g. walking, hiking, or skiing) and limited interaction spaces (e.g. device resting in a pocket).},
	urldate = {2014-03-17},
	booktitle = {Proceedings of the 14th International Conference on Human-computer Interaction with Mobile Devices and Services},
	publisher = {{ACM}},
	author = {Pielot, Martin and Kazakova, Anastasia and Hesselmann, Tobias and Heuten, Wilko and Boll, Susanne},
	year = {2012},
	keywords = {handheld devices and mobile computing, haptic uis, input and interaction technologies, tactile \&\#38},
	pages = {327–330},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/PEEG533I/Pielot et al. - 2012 - PocketMenu Non-visual Menus for Touch Screen Devi.pdf:application/pdf}
},

@inproceedings{bolt_put-that-there:_1980,
	address = {New York, {NY}, {USA}},
	series = {{SIGGRAPH} '80},
	title = {{"Put-that-there":} Voice and Gesture at the Graphics Interface},
	isbn = {0-89791-021-4},
	shorttitle = {{\&Ldquo;Put-that-there\&Rdquo;}},
	url = {http://doi.acm.org/10.1145/800250.807503},
	doi = {10.1145/800250.807503},
	abstract = {Recent technological advances in connected-speech recognition and position sensing in space have encouraged the notion that voice and gesture inputs at the graphics interface can converge to provide a concerted, natural user modality. The work described herein involves the user commanding simple shapes about a large-screen graphics display surface. Because voice can be augmented with simultaneous pointing, the free usage of pronouns becomes possible, with a corresponding gain in naturalness and economy of expression. Conversely, gesture aided by voice gains precision in its power to reference.},
	urldate = {2014-03-18},
	booktitle = {Proceedings of the 7th Annual Conference on Computer Graphics and Interactive Techniques},
	publisher = {{ACM}},
	author = {Bolt, Richard A.},
	year = {1980},
	keywords = {Gesture, Graphics, Graphics interface, Man-machine interfaces, Space sensing, Spatial data management, speech input, Voice input},
	pages = {262–270},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/6FQNISPC/Bolt - 1980 - &Ldquo;Put-that-there&Rdquo; Voice and Gesture at.pdf:application/pdf}
},

@inproceedings{zhang_real-time_2010,
	address = {New York, {NY}, {USA}},
	series = {{ICMI-MLMI} '10},
	title = {Real-time Adaptive Behaviors in Multimodal Human-avatar Interactions},
	isbn = {978-1-4503-0414-6},
	url = {http://doi.acm.org/10.1145/1891903.1891909},
	doi = {10.1145/1891903.1891909},
	abstract = {Multimodal interaction in everyday life seems so effortless. However, a closer look reveals that such interaction is indeed complex and comprises multiple levels of coordination, from high-level linguistic exchanges to low-level couplings of momentary bodily movements both within an agent and across multiple interacting agents. A better understanding of how these multimodal behaviors are coordinated can provide insightful principles to guide the development of intelligent multimodal interfaces. In light of this, we propose and implement a research framework in which human participants interact with a virtual agent in a virtual environment. Our platform allows the virtual agent to keep track of the user's gaze and hand movements in real time, and adjust his own behaviors accordingly. An experiment is designed and conducted to investigate adaptive user behaviors in a human-agent joint attention task. Multimodal data streams are collected in the study including speech, eye gaze, hand and head movements from both the human user and the virtual agent, which are then analyzed to discover various behavioral patterns. Those patterns show that human participants are highly sensitive to momentary multimodal behaviors generated by the virtual agent and they rapidly adapt their behaviors accordingly. Our results suggest the importance of studying and understanding real-time adaptive behaviors in human-computer multimodal interactions.},
	urldate = {2014-01-06},
	booktitle = {International Conference on Multimodal Interfaces and the Workshop on Machine Learning for Multimodal Interaction},
	publisher = {{ACM}},
	author = {Zhang, Hui and Fricker, Damian and Smith, Thomas G. and Yu, Chen},
	year = {2010},
	keywords = {embodied agent, multimodal interaction, virtual human, visualization},
	pages = {4:1–4:8},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/UHVMCTEU/Zhang et al. - 2010 - Real-time Adaptive Behaviors in Multimodal Human-a.pdf:application/pdf}
},

@inproceedings{morimoto_recognition_1996,
	title = {Recognition of Head Gestures Using Hidden Markov Models},
	abstract = {This paper explores the use of Hidden Markov Models ({HMMs)} for the recognition of head gestures. A gesture corresponds to a particular pattern of head movement. The facial plane is tracked using a parameterized model and the temporal sequence of three image rotation parameters are used to describe four gestures. A dynamic vector quantization scheme was implemented to transform the parameters into suitable input data for the {HMMs.} Each model was trained by the iterative Baum-Welch procedure using 28 sequences taken from 5 persons. Experimental results from a different data set (33 new sequences from 6 other persons) demonstrate the effectiveness of this approach. 1. Introduction  The analysis of human gestures, postures and expressions can facilitate better human-computer interaction. Unfortunately, isolating the roles of individual body-parts and their actions turns out to be a difficult task [5]. In this paper we focus on the recognition of a subset of head gestures. Automatic face se...},
	booktitle = {In Proceeding of {ICPR}},
	author = {Morimoto, Carlos and Yacoob, Yaser and Davis, Larry},
	year = {1996},
	pages = {461–465},
	file = {Citeseer - Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/UW7J7G4B/Morimoto et al. - 1996 - Recognition of Head Gestures Using Hidden Markov M.pdf:application/pdf;Citeseer - Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/PPJM8AK4/summary.html:text/html}
},

@inproceedings{dong_recognition_2007,
	title = {Recognition of Multi-Pose Head Gestures in Human Conversations},
	doi = {10.1109/ICIG.2007.176},
	abstract = {We address the problem of recognizing multi-pose head nodding and shaking gestures in human conversations. Existing methods mainly recognize head gestures in restricted environments like human robot interaction, where face poses are near frontal and head motions are not natural. However, in human conversations, faces of subjects might be in arbitrary poses while head gestures are often subtle. Since the face pose change and head gesture movement are of different scale, we propose to track the face of varied poses with a mixed-state particle filter and detect the subtle head movement by a Kanade-Lucas-Tomasi tracker. The motion patterns in both horizontal and vertical directions are detected and then head gestures are analyzed by a Finite State Machine. Experiments on natural human conversations demonstrated the effectiveness of our method.},
	booktitle = {Fourth International Conference on Image and Graphics, 2007. {ICIG} 2007},
	author = {Dong, Ligeng and Jin, Yuxin and Tao, Linmi and Xu, Guangyou},
	year = {2007},
	keywords = {Automata, Face detection, face recognition, finite state machine, finite state machines, gesture recognition, head gesture movement, human computer interaction, human conversations, human robot interaction, Kanade-Lucas-Tomasi tracker, Magnetic heads, mixed-state particle filter, Motion analysis, Motion detection, multipose head gestures recognition, particle filtering (numerical methods), Particle filters, Particle tracking, Pattern analysis, robot vision, shaking gestures},
	pages = {650--654},
	file = {IEEE Xplore Abstract Record:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/2XRU37B8/articleDetails.html:text/html}
},

@inproceedings{smus_running_2010,
	address = {New York, {NY}, {USA}},
	series = {Ubicomp '10 Adjunct},
	title = {Running Gestures: Hands-free Interaction During Physical Activity},
	isbn = {978-1-4503-0283-8},
	shorttitle = {Running Gestures},
	url = {http://doi.acm.org/10.1145/1864431.1864473},
	doi = {10.1145/1864431.1864473},
	abstract = {This paper presents Running Gestures, an interaction technique that relies on foot gestures while running. A prototype and evaluation of one of the proposed gestures, a mid-stride skip, is presented in detail. The developed prototype is used by runners to change the currently playing music track, and the evaluation compares users' performance in relation to other methods of changing tracks while running. The results show that Running Gestures is a highly effective way of interacting with a system when running.},
	urldate = {2014-02-06},
	booktitle = {Proceedings of the 12th {ACM} International Conference Adjunct Papers on Ubiquitous Computing - Adjunct},
	publisher = {{ACM}},
	author = {Smus, Boris and Kostakos, Vassilis},
	year = {2010},
	keywords = {foot gestures, interaction technique, music player, running},
	pages = {433–434},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/68RSEB83/Smus and Kostakos - 2010 - Running Gestures Hands-free Interaction During Ph.pdf:application/pdf}
},

@inproceedings{geelhoed_safety_2000,
	address = {London, {UK}, {UK}},
	series = {{HUC} '00},
	title = {Safety and Comfort of Eyeglass Displays},
	isbn = {3-540-41093-7},
	url = {http://dl.acm.org/citation.cfm?id=647986.743887},
	abstract = {An eyeglass display features two micro displays and both eyes are presented with the same image. This configuration is safer than virtual reality helmets, which give rise to severe vision problems and nausea. They are also safer than monocular displays, which impair judgement of distance, speed and size. Current eyeglass display products are occluded and are likely to produce vergence lock, a potential health hazard. We suggest that eyeglass displays should allow good peripheral vision and should be used in relatively light environments to counteract vergence lock.},
	urldate = {2014-01-07},
	booktitle = {Proceedings of the {2Nd} International Symposium on Handheld and Ubiquitous Computing},
	publisher = {Springer-Verlag},
	author = {Geelhoed, Erik and Falahee, Marie and Latham, Kezzy},
	year = {2000},
	pages = {236–247}
},

@article{rocchesso_sounding_2003,
	title = {Sounding objects},
	volume = {10},
	issn = {1070-{986X}},
	doi = {10.1109/MMUL.2003.1195160},
	abstract = {Interactive systems, virtual environments, and information display applications need dynamic sound models rather than faithful audio reproductions. This implies three levels of research: auditory perception, physics-based sound modeling, and expressive parametric control. Parallel progress along these three lines leads to effective auditory displays that can complement or substitute visual displays. This article aims to shed some light on how psychologists, computer scientists, acousticians, and engineers can work together and address these and other questions arising in sound design for interactive multimedia systems.},
	number = {2},
	journal = {{IEEE} {MultiMedia}},
	author = {Rocchesso, D. and Bresin, R. and Fernstrom, M.},
	month = apr,
	year = {2003},
	keywords = {audio user interfaces, auditory displays, auditory perception, auditory-enhanced interfaces, Biological system modeling, expressive parametric control, Frequency modulation, human factors, Humans, Instruments, interactive multimedia systems, interactive systems, multimedia computing, Psychoacoustic models, Psychology, Shape, Signal generators, sound design, sound modeling},
	pages = {42--52},
	file = {IEEE Xplore Abstract Record:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/QBCGFMCX/abs_all.html:text/html;IEEE Xplore Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/F2DWUD2C/Rocchesso et al. - 2003 - Sounding objects.pdf:application/pdf}
},

@techreport{walker_spatial_2000,
	title = {Spatial audio in small screen device displays},
	abstract = {Our work addresses problem of (visual) clutter in mobile device interfaces. The solution we propose involves the translation of techniques- from the graphical to the audio domain- for exploiting space in information representation. This article presents an illustrative example in the form of a spatialized audio progress bar. In usability tests, participants performed background monitoring tasks significantly more accurately using this spatialized audio (vs. a conventional visual) progress bar. Moreover, their performance in a simultaneously running, visually demanding foreground task was significantly improved in the eyes-free monitoring condition. These results have important implications for the design of multi-tasking interfaces for mobile devices. 1},
	institution = {Personal Technologies},
	author = {Walker, Ashley and Brewster, Stephen},
	year = {2000},
	file = {Citeseer - Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/EZM9EAZ2/Walker and Brewster - 2000 - Spatial audio in small screen device displays.pdf:application/pdf;Citeseer - Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/QMGVBWDU/summary.html:text/html}
},

@techreport{huerta_speech_2000,
	title = {Speech Recognition in Mobile Environments},
	abstract = {The growth of cellular telephony combined with recent advances in speech recognition technology results in sizeable potential opportunities for mobile speech recognition applications. Classic robustness techniques that have been previously proposed for speech recognition yield limited improvements of the degradation introduced by idiosyncrasies of the mobile networks. These sources of degradation include distortion introduced by the speech codec as well as artifacts arising from channel errors and discontinuous transmission. In this thesis we focus on characterizing the distortion introduced to the speech signal by the speech codec and we propose methods for reducing the detrimental effect of coding on recognition accuracy. The initial focus of this thesis is on the full rate {GSM} codec ({FRGSM)} . We propose a method to generate recognition features directly from codec parameters. It is shown in this work that by selectively constructing a cepstral feature vector from the {GSM} codec para...},
	author = {Huerta, Juan M.},
	year = {2000},
	file = {Citeseer - Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/2E6XNG46/Huerta - 2000 - Speech Recognition in Mobile Environments.pdf:application/pdf;Citeseer - Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/DHB7VARM/summary.html:text/html}
},

@article{abowd_human_2002,
	title = {The human experience [of ubiquitous computing]},
	volume = {1},
	issn = {1536-1268},
	doi = {10.1109/MPRV.2002.993144},
	abstract = {To address M. Weiser's (ibid., p. 19-25) human-centered vision of ubiquitous computing, the authors focus on physical interaction, general application features and theories of design and evaluation for this new mode of human-computer interaction.},
	number = {1},
	journal = {{IEEE} Pervasive Computing},
	author = {Abowd, {G.D.} and Mynatt, {E.D.} and Rodden, T.},
	month = jan,
	year = {2002},
	keywords = {Computer displays, Computer networks, Computer vision, design theories, evaluation theories, general application features, human experience, human factors, human-centered vision, human-computer interaction mode, Humans, Keyboards, man-machine systems, mobile computing, Personal digital assistants, Pervasive computing, physical interaction, Space technology, ubiquitous computing, User interfaces, wearable computers},
	pages = {48--57},
	file = {IEEE Xplore Abstract Record:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/77HNMG7N/abs_all.html:text/html;IEEE Xplore Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/XIHF3N8X/Abowd et al. - 2002 - The human experience [of ubiquitous computing].pdf:application/pdf}
},

@book{berard_perceptual_1999,
	title = {The Perceptual Window: Head Motion as a new Input Stream},
	shorttitle = {The Perceptual Window},
	abstract = {We introduce a novel interaction technique using head motions to control the location of a window viewpoint within its document space. Head motion is acquired by a non-intrusive head-tracker using computer vision. The tracking technique used in the system, namely correlation matching, is described in order to exhibit its strengths and weaknesses in the context of tightly coupled interaction. The output of the tracker is used in both a rate control interaction and a position control interaction. The benefit is demonstrated by two user studies built around two common {GUI} tasks: navigating in a two-dimensional document space, and moving an object from one place to another in a document. Our system, The Perceptual Window, allows significant improvements in task completion time after a short learning period. {KEYWORDS} Input Devices, Computer Vision, Head Tracking, Multiple Streams of Spatial Input, Interaction Techniques, Perceptual User Interface, Scrolling, Drag and drop {INTRODUCTION} Th...},
	author = {Berard, Francois},
	year = {1999},
	file = {Citeseer - Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/K6CZSZA9/Berard - 1999 - The Perceptual Window Head Motion as a new Input .pdf:application/pdf;Citeseer - Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/EFWFICVC/summary.html:text/html}
},

@book{card_psychology_1983,
	address = {Hillsdale, {NJ}, {USA}},
	title = {The Psychology of Human-Computer Interaction},
	isbn = {0898592437},
	publisher = {L. Erlbaum Associates Inc.},
	author = {Card, Stuart K. and Newell, Allen and Moran, Thomas P.},
	year = {1983}
},

@article{gaver_sonicfinder:_1989,
	title = {The {SonicFinder:} An Interface That Uses Auditory Icons},
	volume = {4},
	issn = {0737-0024},
	shorttitle = {The {SonicFinder}},
	url = {http://dx.doi.org/10.1207/s15327051hci0401_3},
	doi = {10.1207/s15327051hci0401_3},
	abstract = {The appropriate use of nonspeech sounds has the potential to add a great deal to the functionality of computer interfaces. Sound is a largely unexploited medium of output, even though it plays an integral role in our everyday encounters with the world, a role that is complementary to vision. Sound should be used in computers as it is in the world, where it conveys information about the nature of sound-producing events. Such a strategy leads to auditory icons, which are everyday sounds meant to convey information about computer events by analogy with everyday events. Auditory icons are an intuitively accessible way to use sound to provide multidimensional, organized information to users. These ideas are instantiated in the {SonicFinder}, which is an auditory interface I developed at Apple Computer, Inc. In this interface, information is conveyed using auditory icons as well as standard graphical feedback. I discuss how events are mapped to auditory icons in the {SonicFinder}, and illustrate how sound is used by describing a typical interaction with this interface. Two major gains are associated with using sound in this interface: an increase in direct engagement with the model world of the computer and an added flexibility for users in getting information about that world. These advantages seem to be due to the iconic nature of the mappings used between sound and the information it is to convey. I discuss sound effects and source metaphors as methods of extending auditory icons beyond the limitations implied by literal mappings, and I speculate on future directions for such interfaces.},
	number = {1},
	urldate = {2014-01-08},
	journal = {Hum.-Comput. Interact.},
	author = {Gaver, William W.},
	month = mar,
	year = {1989},
	pages = {67–94}
},

@article{salvador_toward_2007,
	title = {Toward Accurate Dynamic Time Warping in Linear Time and Space},
	volume = {11},
	issn = {1088-{467X}},
	url = {http://dl.acm.org/citation.cfm?id=1367985.1367993},
	abstract = {Dynamic Time Warping ({DTW)} has a quadratic time and space complexity that limits its use to small time series. In this paper we introduce {FastDTW}, an approximation of {DTW} that has a linear time and space complexity. {FastDTW} uses a multilevel approach that recursively projects a solution from a coarser resolution and refines the projected solution. We prove the linear time and space complexity of {FastDTW} both theoretically and empirically. We also analyze the accuracy of {FastDTW} by comparing it to two other types of existing approximate {DTW} algorithms: constraints (such as Sakoe-Chiba Bands) and abstraction. Our results show a large improvement in accuracy over existing methods.},
	number = {5},
	urldate = {2014-02-05},
	journal = {Intell. Data Anal.},
	author = {Salvador, Stan and Chan, Philip},
	month = oct,
	year = {2007},
	keywords = {Dynamic time warping, time series, time series alignment, time series similarity},
	pages = {561–580},
	file = {tdm04.pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/G2S28VT4/tdm04.pdf:application/pdf}
},

@inproceedings{pederson_towards_2010,
	address = {New York, {NY}, {USA}},
	series = {{NordiCHI} '10},
	title = {Towards a Model for Egocentric Interaction with Physical and Virtual Objects},
	isbn = {978-1-60558-934-3},
	url = {http://doi.acm.org/10.1145/1868914.1869022},
	doi = {10.1145/1868914.1869022},
	abstract = {Designers of mobile context-aware systems are struggling with the problem of conceptually incorporating the real world into the system design. We present a body-centric modeling framework (as opposed to device-centric) that incorporates physical and virtual objects of interest on the basis of proximity and human perception, framed in the context of an emerging "egocentric" interaction paradigm.},
	urldate = {2014-03-07},
	booktitle = {Proceedings of the 6th Nordic Conference on Human-Computer Interaction: Extending Boundaries},
	publisher = {{ACM}},
	author = {Pederson, Thomas and Janlert, Lars-Erik and Surie, Dipak},
	year = {2010},
	keywords = {interaction paradigm, user interface design},
	pages = {755–758},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/8CWC7MMC/Pederson et al. - 2010 - Towards a Model for Egocentric Interaction with Ph.pdf:application/pdf}
},

@article{graham_use_1999,
	title = {Use of auditory icons as emergency warnings: evaluation within a vehicle collision avoidance application},
	volume = {42},
	issn = {0014-0139},
	shorttitle = {Use of auditory icons as emergency warnings},
	doi = {10.1080/001401399185108},
	abstract = {In the context of emergency warnings, auditory icons, which convey information about system events by analogy with everyday events, have the potential to be understood more quickly and easily than abstract sounds. To test this proposal, an experiment was carried out to evaluate the use of auditory icons for an in-vehicle collision avoidance application. Two icons, the sounds of a car horn and of skidding tyres, were compared with two conventional warnings, a simple tone and a voice saying 'ahead'. Participants sat in an experimental vehicle with a road scene projected ahead, and they were required to brake in response to on-screen collision situations and their accompanying warning sounds. The auditory icons produced significantly faster reaction times than the conventional warnings, but suffered from more inappropriate responses, where drivers reacted with a brake press to a non-collision situation. The findings are explained relative to the perceived urgency and inherent meaning of each sound. It is argued that optimal warnings could be achieved by adjusting certain sound attributes of auditory icons, as part of a structured, user-centred design and evaluation procedure.},
	language = {eng},
	number = {9},
	journal = {Ergonomics},
	author = {Graham, R},
	month = sep,
	year = {1999},
	note = {{PMID:} 10503056},
	keywords = {Accidents, Traffic, Acoustic Stimulation, Adult, Analysis of Variance, Equipment Design, Female, Humans, Male, Protective Devices, Reaction Time},
	pages = {1233--1248}
},

@inproceedings{sandberg_using_2006,
	title = {Using {3D} audio guidance to locate indoor static objects},
	volume = {50},
	url = {http://pro.sagepub.com/content/50/16/1581.short},
	urldate = {2013-11-12},
	booktitle = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	author = {Sandberg, Samuel and H{\textbackslash}aakansson, Calle and Elmqvist, Niklas and Tsigas, Philippas and Chen, Fang},
	year = {2006},
	keywords = {audio navigation},
	pages = {1581–1584},
	file = {3daudioloc.pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/WFT6SGT6/3daudioloc.pdf:application/pdf}
},

@inproceedings{marshall_using_2011,
	address = {New York, {NY}, {USA}},
	series = {{CHI} '11},
	title = {Using Fast Interaction to Create Intense Experiences},
	isbn = {978-1-4503-0228-9},
	url = {http://doi.acm.org/10.1145/1978942.1979129},
	doi = {10.1145/1978942.1979129},
	abstract = {Several emerging strands of {HCI} involve connecting physical exercise activity with digital interactive systems to create intense combined experiences, for example pervasive games, {GPS} based exercise games and 'exertion interfaces'. Many of these systems are mobile, used outside in public, whilst moving quickly through the environment. In this paper, we argue that the combination of moving fast and interacting with a digital system allows us to create a powerfully intense experience for participants, and that key to this is careful attention to the way in which movement is combined with digital content.  We study an interactive art experience in which a person runs whilst listening to poetry. Based on this study and other {HCI} research, we present a framework for mixing physical and interactive content, based on 3 dimensions, which describe ways that a movement activity may itself create intense experiences, followed by a set of tactics for combining intense movement and interactive content.},
	urldate = {2014-03-17},
	booktitle = {Proceedings of the {SIGCHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Marshall, Joe and Benford, Steve},
	year = {2011},
	keywords = {exertion, full body interaction, intensity, mobile, running},
	pages = {1255–1264},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/KP3J7I46/Marshall and Benford - 2011 - Using Fast Interaction to Create Intense Experienc.pdf:application/pdf}
},

@incollection{brewster_using_2000,
	title = {Using non-speech sounds in mobile computing devices.},
	url = {http://researchrepository.napier.ac.uk/3339/},
	urldate = {2014-01-29},
	booktitle = {First Workshop on Human-Computer Interaction with Mobile Devices},
	publisher = {Dept.of Computing Science, University of Glasgow},
	author = {Brewster, Stephen A. and {LePlâtre}, Grégory and Crease, Murray},
	editor = {Johnson, Chris},
	year = {2000},
	keywords = {{QA75} Electronic computers. Computer science},
	pages = {26--29},
	file = {mobile98.pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/J2WJD5A6/mobile98.pdf:application/pdf;Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/RZV2U2XN/3339.html:text/html}
},

@article{brewster_using_1998,
	title = {Using Nonspeech Sounds to Provide Navigation Cues},
	volume = {5},
	issn = {1073-0516},
	url = {http://doi.acm.org/10.1145/292834.292839},
	doi = {10.1145/292834.292839},
	abstract = {This article describes 3 experiments that investigate the possibiity of using structured nonspeech audio messages called earcons to provide navigational cues in a menu hierarchy. A hierarchy of 27 nodes and 4 levels was created with an earcon for each node. Rules were defined for the creation of hierarchical earcons at each node. Participants had to identify their location in the hierarchy by listening to an earcon. Results of the first experiment showed that participants could identify their location with 81.5\% accuracy, indicating that earcons were a powerful method of communicating hierarchy information. One proposed use for such navigation cues is in telephone-based interfaces ({TBIs)} where navigation is a problem. The first experiment did not address the   particular problems of earcons in {TBIs} such as “does the lower quality of sound over the telephone lower recall rates,” “can users remember earcons over a period of time.” and “what effect does training type have on recall?” An experiment was conducted and results showed that sound quality did lower the recall of earcons. However; redesign of the earcons overcame this problem with 73\% recalled correctly. Participants could still recall earcons at this level after a week had passed. Training type also affected recall. With personal training participants recalled 73\% of the earcons, but with purely textual training results were significantly lower. These results show that earcons can provide good navigation cues for {TBIs.} The final experiment  used compound, rather than hierarchical earcons to represent the hierarchy from the first experiment. Results showed that with sounds constructed in this way participants could recall 97\% of the earcons. These experiments have developed our general understanding of earcons. A hierarchy three times larger than any previously created was tested, and this was also the first test of the recall of earcons over time.},
	number = {3},
	urldate = {2014-01-27},
	journal = {{ACM} Trans. Comput.-Hum. Interact.},
	author = {Brewster, Stephen A.},
	month = sep,
	year = {1998},
	keywords = {auditory interfaces, earcons, navigation, nonspeech audio, telephone-based interfaces},
	pages = {224–259},
	file = {ACM Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/6WS9RXG7/Brewster - 1998 - Using Nonspeech Sounds to Provide Navigation Cues.pdf:application/pdf}
},

@article{pascoe_using_2000,
	title = {Using while moving: {HCI} issues in fieldwork environments},
	volume = {7},
	shorttitle = {Using while moving},
	url = {http://dl.acm.org/citation.cfm?id=355329},
	number = {3},
	urldate = {2014-01-08},
	journal = {{ACM} Transactions on Computer-Human Interaction ({TOCHI)}},
	author = {Pascoe, Jason and Ryan, Nick and Morse, David},
	year = {2000},
	pages = {417–437},
	file = {UsingWhileMoving.pdf:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/7S2ZHUXR/UsingWhileMoving.pdf:application/pdf}
},

@inproceedings{ur_reahman_vibrotactile_2009,
	title = {Vibrotactile rendering of head gestures for controlling electric wheelchair},
	doi = {10.1109/ICSMC.2009.5346213},
	abstract = {We have developed a head gesture controlled electric wheelchair system to aid persons with severe disabilities. Realtime range information obtained from a stereo camera is used to locate and segment the face images of the user from the sensed video. We use an Isomap based nonlinear manifold learning map of facial textures for head pose estimation. Our system is a non-contact vision system, making it much more convenient to use. The user is only required to gesture his/her head to command the wheelchair. To overcome problems with a non responding system, it is necessary to notify the user of the exact system state while the system is in use. In this paper, we explore the use of vibrotactile rendering of head gestures as feedback. Three different feedback systems are developed and tested, audio stimuli, vibrotactile stimuli and audio plus vibrotactile stimuli. We have performed user tests to study the usability of these three display methods. The usability studies show that the method using both audio plus vibrotactile response outperforms the other methods (i.e. audio stimuli, vibrotactile stimuli response).},
	booktitle = {{IEEE} International Conference on Systems, Man and Cybernetics, 2009. {SMC} 2009},
	author = {ur Reahman, S. and Raytchev, B. and Yoda, I. and Liu, Li},
	year = {2009},
	keywords = {Cameras, Control systems, disabled person aid, electric wheelchair control, extended Isomap, Face detection, face image segmentation, face recognition, facial texture, feedback, feedback system, gesture recognition, handicapped aids, haptic interfaces, Head, head gesture, head gesture recognition, image segmentation, isomap based nonlinear manifold learning map, Machine vision, medical control systems, Multidimensional Scaling ({MDS)}, noncontact vision system, pose estimation, realtime range information, Rendering (computer graphics), stereo camera, stereo image processing, Usability, vibrotactile rendering, video sensing, video signal processing, wheelchair system, Wheelchairs},
	pages = {413--417},
	file = {IEEE Xplore Abstract Record:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/BAVGQ53J/abs_all.html:text/html;IEEE Xplore Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/I98BSP5J/ur Reahman et al. - 2009 - Vibrotactile rendering of head gestures for contro.pdf:application/pdf}
},

@misc{stewart_boling_voice_2013,
	title = {Voice Action Music Player},
	url = {https://play.google.com/store/apps/details?id=com.hackedcube.vampLite},
	author = {{Stewart Boling}},
	year = {2013}
},

@incollection{hart_workload_1990,
	title = {Workload Assessment and Prediction},
	copyright = {©1990 Springer {Science+Business} Media {B.V.}},
	isbn = {978-94-010-6680-8, 978-94-009-0437-8},
	url = {http://link.springer.com/chapter/10.1007/978-94-009-0437-8_9},
	abstract = {The concept of workload and its relationship to performance is introduced in this chapter. Four categories of workload measurement techniques (ratings, primary and secondary task measures, and physiological indices) are reviewed, examples of each category are described, and their strengths and weaknesses are summarized. The importance of carefully formulating the question which a measure is to address is emphasized, and it is argued that the question should guide the selection of measures. Issues relevant to implementing and interpreting workload measures are discussed and some of the reasons that different measures provide apparently conflicting information about the same situation (i. e., dissociation) are addressed. Finally, the chapter concludes with a brief description of models that can be used to predict workload.},
	language = {en},
	urldate = {2014-02-03},
	booktitle = {Manprint},
	publisher = {Springer Netherlands},
	author = {Hart, Sandra G. and Wickens, Christopher D.},
	editor = {Booher, Harold R.},
	month = jan,
	year = {1990},
	keywords = {Electrical Engineering},
	pages = {257--296},
	file = {Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/U49ZRFZ6/978-94-009-0437-8_9.html:text/html}
},

@inproceedings{dornbush_xpod_2005,
	title = {{XPod} a human activity and emotion aware mobile music player},
	abstract = {With close to 1.5 billion cellular handsets in use and},
	booktitle = {In Proceedings of the International Conference on Mobile Technology, Applications and Systems},
	author = {Dornbush, S. and Fisher, K. and Mckay, K. and Prikhodko, A. and Segall, Z.},
	year = {2005},
	file = {Citeseer - Full Text PDF:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/A7Z3FH86/Dornbush et al. - 2005 - XPod a human activity and emotion aware mobile mus.pdf:application/pdf;Citeseer - Snapshot:/Users/jonashinge/Library/Application Support/Zotero/Profiles/8r4f30g3.default/zotero/storage/X8BS3SDX/summary.html:text/html}
}